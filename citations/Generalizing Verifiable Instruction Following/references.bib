
@inproceedings{mirzadehgsm,
  title={GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author={Mirzadeh, Seyed Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  booktitle={The Thirteenth International Conference on Learning Representations}
}


@inproceedings{Hochlehnert2025ASL,
  title={A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility},
  author={Andreas Hochlehnert and Hardik Bhatnagar and Vishaal Udandarao and Samuel Albanie and Ameya Prabhu and Matthias Bethge},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:277634184}
}


@inproceedings{golchintime,
  title={Time Travel in LLMs: Tracing Data Contamination in Large Language Models},
  author={Golchin, Shahriar and Surdeanu, Mihai},
  booktitle={The Twelfth International Conference on Learning Representations}
}


@inproceedings{roberts2023cutoff,
  title={To the cutoff... and beyond? a longitudinal perspective on llm data contamination},
  author={Roberts, Manley and Thakur, Himanshu and Herlihy, Christine and White, Colin and Dooley, Samuel},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{dominguez2024training,
  title={Training on the test task confounds evaluation and emergence},
  author={Dominguez-Olmedo, Ricardo and Dorner, Florian E and Hardt, Moritz},
  journal={arXiv preprint arXiv:2407.07890},
  year={2024}
}

@article{wallace2024instruction,
  title={The instruction hierarchy: Training llms to prioritize privileged instructions},
  author={Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex},
  journal={arXiv preprint arXiv:2404.13208},
  year={2024}
}

@article{tam2024let,
  title={Let me speak freely? a study on the impact of format restrictions on performance of large language models},
  author={Tam, Zhi Rui and Wu, Cheng-Kuang and Tsai, Yi-Lin and Lin, Chieh-Yen and Lee, Hung-yi and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:2408.02442},
  year={2024}
}

@article{stolfo2024improving,
  title={Improving instruction-following in language models through activation steering},
  author={Stolfo, Alessandro and Balachandran, Vidhisha and Yousefi, Safoora and Horvitz, Eric and Nushi, Besmira},
  journal={arXiv preprint arXiv:2410.12877},
  year={2024}
}


@article{zhou2023instruction,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@inproceedings{bahdanausystematic,
  title={Systematic Generalization: What Is Required and Can It Be Learned?},
  author={Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
  booktitle={International Conference on Learning Representations}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{lambert2024t,
  title={T$\backslash$" ulu 3: Pushing frontiers in open language model post-training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}


@article{jiang2023followbench,
  title={FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models},
  author={Jiang, Yuxin and Wang, Yufei and Zeng, Xingshan and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Wang, Wei},
  journal={CoRR},
  year={2023}
}


@inproceedings{qin2024infobench,
  title={InFoBench: Evaluating Instruction Following Ability in Large Language Models},
  author={Qin, Yiwei and Song, Kaiqiang and Hu, Yebowen and Yao, Wenlin and Cho, Sangwoo and Wang, Xiaoyang and Wu, Xuansheng and Liu, Fei and Liu, Pengfei and Yu, Dong},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={13025--13048},
  year={2024}
}


@inproceedings{sun2023evaluating,
  title={Evaluating Large Language Models on Controlled Generation Tasks},
  author={Sun, Jiao and Tian, Yufei and Zhou, Wangchunshu and Xu, Nan and Hu, Qian and Gupta, Rahul and Wieting, John Frederick and Peng, Nanyun and Ma, Xuezhe},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing}
}


@inproceedings{limeasuring,
  title={Measuring and Controlling Instruction (In) Stability in Language Model Dialogs},
  author={Li, Kenneth and Liu, Tianle and Bashkansky, Naomi and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  booktitle={First Conference on Language Modeling}
}


@article{chung2024scaling,
  title={Scaling Instruction-Finetuned Language Models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={J. Mach. Learn. Res.},
  year={2024}
}


@inproceedings{hupkes2021compositionality,
  title={Compositionality decomposed: how do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={5065--5069},
  year={2021}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{team2025gemma,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}
@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@article{hupkes2023taxonomy,
  title={A taxonomy and review of generalization research in NLP},
  author={Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={10},
  pages={1161--1174},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{zhao2024wildchat,
  title={Wildchat: 1m chatgpt interaction logs in the wild},
  author={Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
  journal={arXiv preprint arXiv:2405.01470},
  year={2024}
}


@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}


@inproceedings{kim-etal-2025-systematic,
    title = "A Systematic Examination of Preference Learning through the Lens of Instruction-Following",
    author = "Kim, Joongwon  and
      Goyal, Anirudh  and
      Zhang, Aston  and
      Xiong, Bo  and
      Hou, Rui  and
      Kambadur, Melanie  and
      Mahajan, Dhruv  and
      Hajishirzi, Hannaneh  and
      Tan, Liang",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.552/",
    pages = "11062--11082",
    ISBN = "979-8-89176-189-6"
}

@article{lior2025wildifeval,
  title={WILDIFEVAL: Instruction Following in the Wild},
  author={Lior, Gili and Yehudai, Asaf and Gera, Ariel and Ein-Dor, Liat},
  journal={arXiv preprint arXiv:2503.06573},
  year={2025}
}


@article{adler2024nemotron,
  title={Nemotron-4 340b technical report},
  author={Adler, Bo and Agarwal, Niket and Aithal, Ashwath and Anh, Dong H and Bhattacharya, Pallab and Brundyn, Annika and Casper, Jared and Catanzaro, Bryan and Clay, Sharon and Cohen, Jonathan and others},
  journal={arXiv preprint arXiv:2406.11704},
  year={2024}
}


@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{Rafailov2023DirectPO,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18290},
  url={https://api.semanticscholar.org/CorpusID:258959321}
}

@article{Ivison2023CamelsIA,
  title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2},
  author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew E. Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hanna Hajishirzi},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.10702},
  url={https://api.semanticscholar.org/CorpusID:265281298}
}


@article{wang2025reinforcement,
  title={Reinforcement learning for reasoning in large language models with one training example},
  author={Wang, Yiping and Yang, Qing and Zeng, Zhiyuan and Ren, Liliang and Liu, Liyuan and Peng, Baolin and Cheng, Hao and He, Xuehai and Wang, Kuan and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2504.20571},
  year={2025}
}


@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}


@article{olmo20242,
  title={2 OLMo 2 Furious},
  author={OLMo, Team and Walsh, Pete and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia, Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt and others},
  journal={arXiv preprint arXiv:2501.00656},
  year={2024}
}

@article{team2024qwen2,
  title={Qwen2 technical report},
  author={Team, Qwen},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@inproceedings{wangverifiable,
  title={Verifiable Format Control for Large Language Model Generations},
  author={Wang, Zhaoyang and Jiang, Jinqi and Zhou, Huichi and Zheng, Wenhao and Zhang, Xuchao and Bansal, Chetan and Yao, Huaxiu},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2025},
  pages={3499--3513},
  year={2025}
}