\begin{table*}[ht!]
    \centering
    \footnotesize
    \begin{tabular}{@{}lclc@{}}
    \toprule
    \multicolumn{1}{l}{\textbf{Name}} & \textbf{Size} & \textbf{License} & \multicolumn{1}{c}{\textbf{Description}} \\ \midrule
    \href{https://huggingface.co/google/gemma-2b-it}{gemma-2b}       & 2b  & Gemma & \multirow{2}{7cm}{Gemma is a family of open models from Google \citep{team2024gemma}}                       \\
    \href{https://huggingface.co/google/gemma-7b-it}{gemma-7b}       & 7b  & Gemma &                                          \\ \midrule
    \href{https://huggingface.co/THUDM/glm-4-9b-chat}{glm-4-9b}       & 9b  & GLM-4 & \parbox{7cm}{GLM-4-9B is an open-source version of the latest generation of pre-trained models launched by Zhipu AI~\citep{du2022glm}.}                                        \\ \midrule
    \href{https://huggingface.co/01-ai/Yi-1.5-9B-Chat}{yi-1.5-9b}       & 9b  & Yi & \multirow{2}{7cm}{Yi series are bilingual language models trained on a 3T multilingual corpus by 01.AI \citep{ai2024yi}}                       \\
    \href{https://huggingface.co/01-ai/Yi-1.5-34B-Chat}{yi-1.5-34b}       & 34b & Yi &                                          \\ \midrule
    \href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{llama-2-7b}       & 7b  & \llama 2 Community & \multirow{3}{7cm}{\llama 2 models are the latest generation developed by Meta AI \citep{touvron2023llama}, pretrained on 2.2T tokens.}                       \\
    \href{https://huggingface.co/meta-llama/Llama-2-13b-chat-hf}{llama-2-13b}       & 13b & \llama 2 Community &                                          \\
    \href{https://huggingface.co/meta-llama/Llama-2-70b-chat-hf}{llama-2-70b}       & 70b & \llama 2 Community &                                          \\
    \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{llama-3-8b}       & 8b  & \llama 3 Community & \multirow{2}{7cm}{\llama 3 are the latest open models from Meta AI \citep{meta_llama_3}, pretrained on 15T tokens.}                       \\
    \href{https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct}{llama-3-70b}       & 70b & \llama 3 Community &                                          \\ 
    \href{https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct}{llama-3.1-8b}       & 8b  & \llama 3.1 Community & \multirow{3}{7cm}{\llama 3.1 collection offers a series of multilingual models that outperform many open and closed chat models on industry benchmarks \citep{dubey2024llama}.}                       \\
    \href{https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct}{llama-3.1-70b}       & 70b & \llama 3.1 Community &                                          \\
    \href{https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct}{llama-3.1-405b}       & 405b & \llama 3.1 Community &                                          \\ \midrule 
    \href{https://huggingface.co/allenai/tulu-2-7b}{\tulu-2-7b}       & 7b  & AI2 ImpACT Low-risk & \multirow{6}{7cm}{\tulu V2 \citep{ivison2023camels} is a series of \llama 2 based models that are instruction-tuned on \tulumix.}                       \\
    \href{https://huggingface.co/allenai/tulu-2-dpo-7b}{\tulu-2-dpo-7b}       & 7b  & AI2 ImpACT Low-risk &                                          \\
    \href{https://huggingface.co/allenai/tulu-2-13b}{\tulu-2-13b}       & 13b & AI2 ImpACT Low-risk &                                          \\
    \href{https://huggingface.co/allenai/tulu-2-dpo-13b}{\tulu-2-dpo-13b}       & 13b & AI2 ImpACT Low-risk &                                          \\
    \href{https://huggingface.co/allenai/tulu-2-70b}{\tulu-2-70b}       & 70b & AI2 ImpACT Low-risk &                                          \\
    \href{https://huggingface.co/allenai/tulu-2-dpo-70b}{\tulu-2-dpo-70b}       & 70b & AI2 ImpACT Low-risk &                                          \\ \midrule
    \href{https://deepmind.google/technologies/gemini/}{gemini-1.0-pro}       & -             & Proprietary & \multirow{3}{7cm}{Gemini models are the most capable multimodal models from Google featuring long context lengths \citep{team2023gemini}.}                       \\
    \href{https://deepmind.google/technologies/gemini/}{gemini-1.5-flash}       & -             & Proprietary &                                          \\
    \href{https://deepmind.google/technologies/gemini/}{gemini-1.5-pro}       & -             & Proprietary &                                          \\ \midrule
    \href{https://huggingface.co/Qwen/Qwen1.5-32B-Chat}{qwen-1.5-32b}       & 32b  & Qianwen & \multirow{4}{7cm}{Qwen is a family of models built by Alibaba Cloud~\citep{qwen}. Qwen1.5 and Qwen2 have recently surpassed most open models on common benchmarks.}                       \\
    \href{https://huggingface.co/Qwen/Qwen1.5-72B-Chat}{qwen-1.5-72b}       & 72b  & Qianwen &                                          \\
    \href{https://huggingface.co/Qwen/Qwen2-72B-Instruct}{qwen-2-72b}       & 72b  & Qianwen &                                          \\
    \href{https://huggingface.co/Qwen/Qwen2.5-72B-Instruct}{qwen-2.5-72b}       & 72b  & Qianwen &                                          \\
        \midrule
    \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{mistral-7b-v0.3}          & 7b  & Apache 2.0 & \multirow{2}{7cm}{Instruction-tuned versions of Mistral models~\citep{jiang2023mistral} from Mistral AI.}                       \\
    % \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}{mistral-7b-v0.2}       & 7b  & Apache 2.0 &                                          \\
    % \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}{mistral-7b-v0.3}       & 7b  & Apache 2.0 &                                          \\
    \href{https://mistral.ai/news/mistral-large/}{mistral-large}       & -             & Proprietary &                                          \\ \midrule
    \href{https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1}{mixtral-8x7b}       & 8x7b & Apache 2.0 & \parbox{7cm}{Mixtral-8x22B is a pretrained generative Sparse Mixture of Experts (MoE) from Mistral AI~\citep{jiang2024mixtral}}                                        \\ \midrule
    \href{https://www.anthropic.com/api}{claude-3-haiku}       & -             & Proprietary & \multirow{3}{7cm}{Claude-3-Haiku and Claude 3 Opus, and Claude 3.5 Sonnet are top proprietary models trained by Anthropic PBC \citep{claude}.}                       \\
    \href{https://www.anthropic.com/api}{claude-3-opus}       & -             & Proprietary &                                          \\
     \href{https://www.anthropic.com/api}{claude-3.5-sonnet}       & -             & Proprietary &                                          \\
        \midrule
    \href{https://platform.openai.com/docs/models}{gpt-3.5-turbo-0125}       & -             & Proprietary & \multirow{3}{7cm}{GPT models are strong proprietary models~\citep{achiam2023gpt} from OpenAI. ``o1'' model was released in September 2024 with strong reasoning capability. }                       \\
             \href{https://platform.openai.com/docs/models}{gpt-4-0613}                   & -             & Proprietary & \\

    \href{https://platform.openai.com/docs/models}{gpt-4o-2024-05-13}                   & -             & Proprietary & \\
     \href{https://platform.openai.com/docs/models}{gpt-4o-2024-08-06}                   & -             & Proprietary & \\
  \href{https://platform.openai.com/docs/models}{o1-mini-2024-0912}                   & -             & Proprietary & \\
 \midrule
    \href{https://huggingface.co/prometheus-eval/prometheus-8x7b-v2.0}{prometheus-2-8x7b}       & 8x7b  & Apache 2.0 & \parbox{7cm}{Prometheus 2 is an alternative to GPT-4 for fine-grained evaluation of LLMs and reward models for RLHF, based on Mistral-Instruct~\citep{kim2024prometheus2}}                                        \\ 
    \href{https://huggingface.co/NCSOFT/Llama-3-OffsetBias-8B}{offsetbias-lm}       & 8b  & \llama 3 Community & \parbox{7cm}{OffsetBias is a generative judge model for pairwise preference evaluation, designed to be robust against various evaluation biases~\citep{park2024offsetbias}}                                        \\ \midrule
    
  \href{https://huggingface.co/nvidia/Nemotron-4-340B-Reward}{nemotron-4-340b-reward}       & 340b  & NVIDIA Open Model & \parbox{7cm}{A multi-aspect reward model for synthetic data generation and RLAIF, based on Nemotron-4-340B-Base~\citep{wang2024helpsteer2}}   \\                                
    \href{https://huggingface.co/NCSOFT/Llama-3-OffsetBias-RM-8B}{offsetbias-rm}       & 8b  & \llama 3 Community & \parbox{7cm}{Reward model trained on OffsetBias dataset, designed to be robust against various evaluation biases~\citep{park2024offsetbias}}                                        \\ 

     \bottomrule
    \end{tabular}
    \caption{Model registry and metadata in our study used in \S\ref{sec:status-quo}.}
    \label{tab:appx_model_registry}
    \end{table*}