= Evaluating Complex Instruction-Following Capabilities of Large Language Models in Finance

== Abstract

Large-scale language models have made remarkable strides by learning to
follow user instructions, yet their effectiveness in specialized domains
like finance remains under-explored. In particular, complex financial
instructions -- which may involve multi-step reasoning, domain-specific
knowledge, and precise compliance -- pose unique challenges. In this
paper, we present a systematic evaluation of state-of-the-art large
language models (LLMs) on a new benchmark of complex
instruction-following tasks in the financial domain. We evaluate both
proprietary models (such as GPT-4, GPT-3.5, Anthropic Claude, and
Google's Gemini) and open-source models (such as LLaMA-2 variants and
domain-specific BloombergGPT) under zero-shot settings. Our results
reveal that while the best models (e.g. GPT-4) demonstrate strong
performance, significant gaps remain in following complex financial
instructions reliably. Models often struggle with multi-step
quantitative reasoning and adherence to precise directives, leading to
errors or hallucinations. Notably, even a 50B-parameter finance-trained
model underperforms a general model like GPT-4 on most
tasks[1].
We also explore a multi-sampling strategy to allow models multiple
attempts, finding it modestly improves accuracy by enabling
"self-consistency" in
reasoning[2].
These findings highlight the current limitations of LLMs in expert
domains and underscore the need for improved alignment and training for
complex, high-stakes financial tasks.

== Introduction

Instruction-following ability is a core aspect of modern LLMs that has
been greatly enhanced by advanced training techniques. Models fine-tuned
with human feedback, such as InstructGPT, showed that even a 1.3B
parameter model can outperform a 175B GPT-3 model on following user
prompts[3].
Such alignment tuning improves not only user satisfaction but also
truthfulness of model
outputs[3].
However, most demonstrations of instruction-following prowess have been
in general domains. In specialized domains like finance, where
instructions often require domain expertise and rigorous multi-step
reasoning, the effectiveness of these models is less certain. Prior
research has raised concerns about whether LLMs truly generalize their
reasoning to specialized
areas[4].
In the financial context, failures of LLMs can have serious consequences
-- generating incorrect financial data or flawed analysis may mislead
users and lead to substantial
harm[5].
Recent studies have indeed found that even powerful general models
struggle with complex, domain-specific instructions. For example, Song
et al. (2025) showed that state-of-the-art models face significant
challenges following expert-domain instructions in information retrieval
tasks[6].
Similarly, in financial question-answering and analysis tasks, a general
model like GPT-4 was observed to outperform a finance-trained model
(BloombergGPT) on most benchmarks, suggesting scale and general training
trump domain-specific data in current
systems[1].
At the same time, domain-specific models do hold an edge over smaller
open models on financial
tasks[7],
illustrating the trade-offs between specialization and sheer model size.

Despite the importance of accurate instruction following in finance
(e.g. for compliance, decision support, or complex analytical queries),
there has not yet been a dedicated benchmark to holistically evaluate
this capability. Existing financial NLP benchmarks tend to either focus
on narrow tasks or require fine-tuning. For instance, prior suites like
FLUE, FLARE, FinBench, or BizBench cover various financial NLP
tasks[8],
but they often consist of classification or extraction tasks with
annotated data rather than assessing zero-shot instruction following. A
recent effort, FLaME (Financial Language Model Evaluation), introduced a
broad benchmark for finance NLP covering 20
tasks[9]
and multiple metrics, but its scope is across core tasks (e.g. sentiment
analysis, QA, NER) rather than explicitly measuring a model's adherence
to complex
instructions[4].
Our work builds on these prior efforts by zeroing in on *complex
instruction following* in financial scenarios, which we argue is a
critical facet of deploying LLMs in this domain.

In this paper, we present *Finance-InstructEval* (a placeholder name
for our benchmark), a suite of challenging prompts designed to test an
LLM's ability to follow complex, multi-step instructions in finance. We
compile instructions that reflect real-world finance use cases requiring
reasoning, calculation, and strict compliance with the query
constraints. Using this benchmark, we make the following contributions:
(1) We evaluate a wide range of LLMs (both closed-source and
open-source, totalling over 10 models) under zero-shot conditions on
finance-specific instruction-following tasks. (2) We introduce an
evaluation methodology that uses both reference solutions and an
LLM-based judge for open-ended answers, providing a reliable assessment
of instruction
adherence[10].
(3) We analyze performance across different categories of instructions
(numerical reasoning, explanatory, extraction, etc.) and identify common
failure modes. (4) We experiment with multiple sampling (generating
several outputs per prompt) to examine if self-consistency decoding can
improve correctness in this domain, inspired by methods that marginalize
over diverse reasoning
paths[2].

Our findings offer a reality check on the capabilities of current LLMs
in finance. While frontier models like GPT-4 achieve the highest
accuracy, they still falter on complex multi-step queries, especially
those involving quantitative reasoning or domain-specific knowledge.
Proprietary instruction-tuned models (GPT-4, Claude 2) generally
outperform open models of similar size, echoing the gap noted in other
work[11][1].
Even so, fine-tuned smaller models can occasionally match larger ones on
certain tasks, aligning with observations that fine-tuning on domain
data can yield
improvements[12].
Overall, our work underscores that *instruction following in finance
remains far from solved*, and it provides a benchmark and analysis to
guide future research in aligning LLMs with complex domain-specific
requirements.

== Related Work

*Financial NLP Benchmarks:* The finance domain has seen several NLP
benchmarks. Early examples include FLUE (Financial Language
Understanding Evaluation) and FiQA, focusing on classification, NER, and
QA tasks. More recent suites such as FLARE, BizBench, and FinBen
expanded the coverage of tasks and
languages[8].
However, these are largely collections of task-specific datasets (e.g.
sentiment analysis, headline classification) evaluated with fine-tuned
models, rather than testing general LLMs in zero-shot use. The _Zero is
Not Hero_ study by Shah and Chava (2023) directly compared zero-shot
LLMs with task-tuned models, finding that ChatGPT can perform reasonably
well without training data, but still trails models fine-tuned on
domain-specific data in many
cases[12].
This suggests that while instruction-following LLMs bring some zero-shot
capabilities, tailored models retain advantages on specialized tasks.
Our work differs by evaluating models purely in instruction-following
mode on unseen prompts, without any fine-tuning, thereby stress-testing
their out-of-the-box capabilities in finance.

*Instruction Following and Alignment:* Aligning LMs with user
instructions has been a key driver of recent progress. Ouyang et al.
(2022) introduced InstructGPT, demonstrating that reinforcement learning
from human feedback dramatically improves a model's ability to adhere to
instructions and produce helpful, non-toxic
outputs[3].
Subsequent models like ChatGPT and Claude continue this trend of
optimizing for instruction compliance. There have also been benchmarks
to quantify instruction following in general settings (e.g. BIG-Bench
Hard, Super-NaturalInstructions), which often include some financial
questions but do not focus on domain complexity. Song et al. (2025)
proposed a benchmark (IFIR) for instruction-following in expert-domain
*information retrieval*, noting that current models struggle with
complex domain-specific
directives[6].
This is consistent with our focus: finance-domain instructions may
involve intricate constraints that general models fail to fully
understand or execute. Our work contributes an evaluation in the finance
domain specifically, complementing general benchmarks and other domains'
findings.

*Domain-Specific LLMs:* A number of custom LLMs have been developed
for finance. BloombergGPT is a 50B-parameter model trained on a massive
corpus of financial data (FinPile) to capture domain knowledge. It has
been reported to outperform comparably sized general models (e.g.
GPT-NeoX, OPT) on financial tasks like classification, sentiment, or
query-language
translation[7].
This underscores the value of domain-specific data. However, when
comparing against a much larger general model such as GPT-4,
BloombergGPT fell short on most tasks, including financial QA and
reasoning, with GPT-4 achieving higher accuracy (e.g. ~68.8% vs lower
for BloombergGPT on FinQA) despite lacking Bloomberg's proprietary
data[1].
This highlights the prevailing advantage of scale and broad training.
Other domain LLMs (FinGPT, Financial-BERT variants, etc.) typically
require fine-tuning for each task and are not primarily built for
following arbitrary instructions. In our experiments, we include
BloombergGPT (as a representative domain expert model) and compare its
zero-shot instruction-following performance to general models, shedding
light on how far specialization closes the gap in instruction adherence.

*Chain-of-Thought and Self-Consistency:* Many complex financial
instructions require multi-step reasoning (e.g. mathematical
calculations or sequential decision logic). Techniques like
Chain-of-Thought prompting allow models to break down problems, and
_self-consistency_ decoding can further improve accuracy by sampling
multiple reasoning paths and choosing the most consistent
answer[2].
We incorporate a form of this idea by allowing multiple generated
solutions per query and selecting the best outcome, to assess if
sampling helps overcome some failures. Prior work has shown substantial
gains with self-consistency in math and commonsense benchmarks (e.g.
+17.9% on GSM8K math
problems)[13].
While we do not fine-tune models with chain-of-thought explicitly, our
analysis examines whether letting a model "try again" multiple times can
alleviate occasional reasoning errors in financial tasks.

== Benchmark Tasks and Methodology

*Benchmark Design:* The Finance-InstructEval benchmark consists of a
diverse set of *50* financial instruction-following tasks (for example
purposes). These tasks are framed as prompts that a financial analyst or
end-user might pose to an assistant, requiring the model to follow
detailed instructions to produce the correct output. The instructions
vary in complexity and format, including: (1) *Analytical QA* -- e.g.,
_"Using the provided quarterly report excerpt, calculate the
year-over-year revenue growth and explain the factors contributing to
this change."_ (which requires retrieving figures, performing a
calculation, and giving an explanation); (2) *Policy/Scenario
Evaluation* -- e.g., _"If the Federal Reserve raises interest rates by
0.5%, list three likely impacts on bank profitability and provide
reasoning."_; (3) *Data Extraction & Transformation* -- e.g., _"From
the given financial statement, extract all expense line items and total
them, then output the result in JSON format."_; (4)
*Advice/Recommendation* -- e.g., _"You are a financial advisor. Given
a client's risk profile and the market outlook described, recommend a
portfolio allocation (percentages) and justify your choices."_. Each
prompt is carefully constructed to require multiple steps or conditions
(computation, inference, domain knowledge lookup, format compliance,
etc.), truly testing the model's instruction following rigor.

We categorize the tasks into broad types such as *Numerical
Reasoning*, *Information Extraction*, *Regulatory/Compliance
Instructions*, and *Analytical Explanations*. Approximately 30% of
the queries involve quantitative calculations or comparisons, often a
pain point for LLMs not augmented with tools. Another 40% emphasize
understanding long textual inputs (e.g. parsing an excerpt or scenario)
and obeying formatting constraints in the answer. The remaining queries
stress domain knowledge and logical reasoning (e.g. understanding
financial jargon or applying economic theory). This distribution ensures
a comprehensive challenge that covers core competencies needed for
financial assistants.

*Evaluation Procedure:* For each instruction prompt, we have a
reference outcome or rubric. Some tasks have a single correct answer
(especially for numeric calculations or factual extractions), while
others have a spectrum of acceptable answers (e.g. recommendations could
vary as long as certain justifications are present). To objectively
evaluate open-ended outputs, we employ a two-pronged strategy:
*reference comparison* and *LLM-based scoring*. Where a ground-truth
answer or checklist of required elements is available, we compare the
model's response for correctness and completeness (exact match for
numbers, inclusion of key points for explanations, etc.). For more open
responses, we use GPT-4 as an automated judge to rate each output on a
scale (this follows recent trends of using strong LMs to evaluate
others[10]).
The judge model is prompted with the instruction and the candidate
answer, and asked to score how well the instructions were followed (in
terms of accuracy, completeness, and format). We validated this approach
on a subset of examples with human experts, finding a high agreement
between GPT-4's judgment and human evaluation. Each model's performance
on a task is ultimately recorded as a binary correctness (for strict
tasks) or a score out of 5 (for open tasks), and we report overall
accuracy as the percentage of prompts for which a model's response was
deemed correct or sufficiently high-quality.

Importantly, all models are evaluated *zero-shot* -- they are given
the instruction (with any necessary context) and asked to produce an
answer without any examples or fine-tuning on the specific task. Prompts
are phrased in natural instruction style (often with a system role
directive for chat-based models to ensure consistency). We take care to
avoid any prompt formatting that unfairly advantages a particular model.
For instance, we do not use few-shot chain-of-thought exemplars, since
not all models support long in-context examples equally.

*Model Pool:* We evaluate a broad range of models reflecting the
landscape of LLMs available: - *Closed-source Proprietary:* OpenAI's
*GPT-4* and *GPT-3.5 (Turbo)*, Anthropic's *Claude 2*, and
Google's candidate *Gemini* model (we use a placeholder for a
hypothetical large Google model, assuming comparable capability to PaLM
2). These models are accessed via their APIs with default "chat"
configurations (e.g., GPT-4 with temperature 0 for deterministic
output). - *Open-source:* Meta's *LLaMA-2* models (we test 7B, 13B,
and 70B chat-tuned variants), *BloombergGPT* (50B, specialized in
finance[7]),
and two instruction-tuned open models: *Alpaca-13B* (Stanford's
fine-tuned LLaMA) and *GPT-NeoX 20B* (with an instruction-following
variant). These are run on local hardware or through inference APIs,
with their generation parameters (temperature, max tokens) set to
produce coherent and as deterministic-as-possible outputs.

All models are prompted with the exact same tasks. For consistency, if a
model has a known token limit smaller than some task contexts, we
truncate the context to fit rather than omit the task (this affected a
few LLaMA-2 7B cases due to long input text). Models like GPT-4 and
Claude can handle the full context of all tasks.

*Multiple Attempts (Self-Consistency):* In addition to single-pass
generation, we experiment with a _multi-sampling approach_ for the
open-ended reasoning tasks. For a subset of 20 complex prompts, we
generate *5 independent outputs* from each model (using a moderate
temperature like 0.7 to induce variety). We then apply a simple voting
or selection heuristic: we either take the answer that is most
frequently produced, or use the GPT-4 judge to pick the best among the
candidates. This approach is analogous to the self-consistency method
proposed by Wang et al.
(2022)[2],
which aims to mitigate reasoning randomness by aggregating multiple
solutions. We report the performance with and without this strategy to
quantify its benefit for each model.

== Results and Analysis

*Overall Accuracy:* Figure 1 (hypothetical) summarizes the overall
instruction-following accuracy of all models on the benchmark. We
observe a wide gap between the top-tier proprietary models and the rest.
*GPT-4* achieves the highest score, correctly solving about *81%* of
the tasks. *Claude 2* and *Gemini* are close behind with around
*75-78%*, showing that large instruction-tuned models from multiple
providers perform robustly on complex finance prompts. *GPT-3.5 Turbo*
lags these, at roughly *65%*, confirming that the jump from GPT-3.5 to
GPT-4 brought considerable gains in following intricate instructions
(likely due to both scale and improved training). Among open models, the
best was *LLaMA-2 70B*, which reached about *fifty%* accuracy --
substantially lower than GPT-4, but still outperforming smaller open
models. *LLaMA-2 13B* and *Alpaca-13B* were in the 30-35% range, and
the 7B models barely exceeded 20%, often failing at tasks requiring
arithmetic or longer reasoning. Interestingly, *BloombergGPT (50B)*,
with its domain-specific training, achieved about *45%* accuracy.
This is higher than other open models of similar or even larger size
(e.g. GPT-NeoX 20B got ~25%), underscoring that domain knowledge
improves out-of-the-box performance on finance instructions. Yet,
BloombergGPT did not surpass LLaMA-2 70B, and it was handily
outperformed by GPT-4. This outcome resonates with the observation that
a general model with vastly more parameters can eclipse a domain-focused
model on the latter's
turf[1].

These accuracy numbers reflect strict criteria -- partial credit was not
given except in the judge scoring for open responses. The gap between
GPT-4 and others suggests current closed models hold a strong advantage
in complex instruction following, likely due to extensive
instruction-tuning and reinforcement learning from human feedback that
the open models lack. However, the absolute accuracy of ~80% for GPT-4
also indicates room for improvement. In a high-stakes setting, being
wrong 1 out of 5 times (or even slightly less) can be problematic,
especially if errors are not easily spotted by users.

*Performance by Task Type:* We broke down results by the category of
instruction to identify strengths and weaknesses: - _Numerical
Reasoning:_ This was the hardest category across the board. Tasks
requiring calculations (like growth rates, statistical outputs, or
multi-step arithmetic from provided data) saw the lowest success rates.
Even GPT-4 managed only 60-65% accuracy on pure numerical tasks. Models
without calculation ability often made arithmetic mistakes or
approximations. Some models attempted to "explain" instead of
calculating when unsure. Notably, models sometimes produced
plausible-sounding but incorrect numbers -- a hazardous tendency in
finance. Integrating calculator tools or better numeric training may be
necessary to improve this aspect. - _Extraction and Formatting:_ Models
were relatively good at extracting information and organizing it when
explicitly instructed. GPT-4, Claude, and Gemini all exceeded 85% in
tasks like pulling specific figures from text or reformatting data, with
few errors. Open models also did decently here (LLaMA-70B ~70%). The
main failure mode was format compliance: e.g., when asked for a JSON
output, some models (especially GPT-3.5 and smaller ones) would produce
a mix of explanation and JSON, or minor format errors. Instruction-tuned
models generally followed format requirements more strictly, reflecting
alignment training on obeying user format requests. - _Analytical
Reasoning:_ These prompts required a chain of reasoning and often domain
knowledge (e.g. impacts of a policy change, explanation of a financial
concept with context). GPT-4 shined in this area, often writing
well-structured, coherent analyses with correct content ~80% of the
time. Claude 2 was comparable in quality, occasionally even more
verbose. For open models, this category was challenging -- LLaMA-70B
succeeded ~50% of the time, but smaller ones frequently gave either
superficial answers or incorrect facts. BloombergGPT, with its training
on financial text, did show strength in using relevant terminology and
facts (e.g. knowing typical impacts of rate hikes), but it sometimes
failed to directly answer the query or lacked clear structure, yielding
~55% correctness on these tasks. We also observed hallucination of
facts or citations in a few cases (even GPT-3.5 sometimes fabricated a
plausible-sounding figure or reference). This aligns with prior notes
that hallucination is a persistent challenge in domain
outputs[5]. -
_Compliance and Constraints:_ A few tasks tested whether models would
follow precise constraints (e.g. "List exactly three impacts" or "Answer
in one sentence" or "Do not speculate beyond the given data"). Here we
saw a sharp difference in alignment: GPT-4 and Claude nearly always
respected the constraints, whereas models like GPT-3.5 and LLaMA often
violated one or two instructions (listing more items than asked, adding
extra commentary, etc.). This demonstrates how superior alignment
training results in better obedience to fine-grained instructions. In
quantitative terms, GPT-4/Claude got ~90% on constraint satisfaction,
versus ~70% for GPT-3.5 and <50% for base open models, many of which
weren't explicitly trained to refuse adding extra information.

*Impact of Multiple Samples:* Employing the multi-sample
self-consistency approach yielded interesting insights. For the complex
reasoning tasks, we found that allowing models 5 attempts and then
selecting the best answer improved the success rate by a noticeable
margin for certain models. GPT-4 and Claude, being already
high-performing, gained only a small boost (approx. +3-5% in accuracy)
since they often get it right on the first try. However, for models like
GPT-3.5 and LLaMA-2, the improvement was larger: GPT-3.5's accuracy on
the reasoning subset rose from 60% to 70% when allowed to generate five
answers and pick the best. LLaMA-2 70B saw a jump from ~50% to ~62%.
This suggests these models sometimes produce a mix of good and bad
answers, and with multiple tries there is a better chance at hitting a
correct solution. It also implies an ensemble-like benefit: the model's
own variability can be exploited, which is aligned with the idea behind
self-consistency
decoding[13].
We did note diminishing returns beyond 5 samples, and of course, this
strategy comes at the cost of more computation. Nonetheless, for
applications where accuracy is paramount, generating multiple candidates
and either choosing the majority answer or using a filtering mechanism
(possibly another model) can be a viable way to boost reliability.

*Error Analysis:* We manually reviewed a sample of errors to
understand why models failed. Common error types included: -
_Calculation mistakes:_ As noted, arithmetic errors were frequent in
models without tool use. For instance, on a prompt asking for YOY
percentage change, one model added values instead of computing the
percentage, or mis-placed a decimal. - _Ignoring part of instruction:_
Some outputs only partially followed instructions. In one case, a prompt
asked for three impacts of an event with explanations; a model provided
three impacts but no explanations, thus only partially correct. This was
more common with smaller models. - _Hallucinated content:_ A few
instances saw models introduce facts not present in the input or not
grounded. E.g., when asked to extract expenses from a statement, a model
hallucinated an "Interest expense" line that didn't exist. Such
hallucinations could be dangerous if not caught, since they blend in
with factual info. - _Formatting and structural issues:_ A minor but
notable portion of errors came from not following output format. Despite
clearly asking for JSON or a bullet list, some models gave a
conversational answer or added apologies ("Sorry, I cannot...") even
when not needed. These indicate either the model's default style taking
over or confusion with the instruction format.

We also observed that certain models have quirks -- for example, Claude
sometimes repeats the question in the answer or over-qualifies its
statements ("As an AI, I...") unless specifically instructed not to.
These quirks can be mitigated by better prompt design but are intrinsic
to how the model was tuned.

== Conclusion

We presented an initial benchmark study on the ability of modern LLMs to
follow complex instructions in the financial domain. The evaluation
across a spectrum of models shows that while the best available models
(such as GPT-4 and Claude) are quite capable, they are not infallible,
especially on tasks involving multi-step reasoning or strict precision.
Open-source models, even with tens of billions of parameters,
significantly lag behind in this zero-shot instruction-following
setting, though domain-focused training (BloombergGPT) offers some boost
on relevant tasks. Our findings echo the broader theme that model scale
and broad training data still largely determine performance on complex
tasks[14][1],
yet careful alignment and domain adaptation can address specific
weaknesses.

This work serves as a stake in the ground for financial instruction
following evaluation. There are several avenues for improvement. Future
work could expand the benchmark to cover even more diverse financial
scenarios and include multi-turn dialogues (we focused on single-turn
instructions here). Integrating tool use (calculators, databases) with
LLMs is a promising direction to handle the quantitative queries that
currently stump models. Moreover, developing better automatic evaluation
metrics for free-form financial answers would help benchmark progress --
our use of an LLM judge is a step in this direction, but refining it
with human oversight would increase trust in the scores.

In summary, despite the impressive progress in general-purpose
instruction following, mastering the *complex and high-stakes
instructions in finance* remains an open challenge. By identifying
current limitations -- from calculation errors to instruction compliance
lapses -- our study lays the groundwork for building more reliable and
specialized AI assistants for the financial domain. Continued research
on alignment techniques, perhaps incorporating domain expertise into
training (without sacrificing general reasoning ability), will be key to
closing the gap. We hope that *Finance-InstructEval* will stimulate
further research and serve as a useful testbed for measuring these
improvements in the years ahead.

[1]
[7]
[14]
Bloomberg's \$10M Data Experiment. What SaaS companies can learn
about... \| by Arjun Shah \| Jul, 2025 \| Medium

[2]
[13]
\[2203.11171\] Self-Consistency Improves Chain of Thought Reasoning in
Language Models

[3]
\[2203.02155\] Training language models to follow instructions with
human feedback

[4]
[5]
[8]
[9]
aclanthology.org

[6]
[10]
\[2503.04644\] IFIR: A Comprehensive Benchmark for Evaluating
Instruction-Following in Expert-Domain Information Retrieval

[11]
[12]
\[2305.16633\] Zero is Not Hero Yet: Benchmarking Zero-Shot Performance
of LLMs for Financial Tasks