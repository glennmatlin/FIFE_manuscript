diff --git a/content/08_recommendations.typ b/content/08_recommendations.typ
index 3a3b2c1..5d6e7f8 100644
--- a/content/08_recommendations.typ
+++ b/content/08_recommendations.typ
@@ -5,7 +5,7 @@
 *Comparison to baselines.* Establishing control conditions using deterministic agents or human SME players or adjudicators enables qualitative and quantitative measurements of LM agent performance in various conditions, and can help detect systematic biases or failure modes unique to LM reasoning. Existing human baselines in relevant task spaces (e.g. creative writing, strategic deception) are largely neither transparent nor rigorous enough to provide meaningful comparisons @wei_recommendations_2025. High-stakes wargames therefore should prioritize bespoke evaluations with scenario-relevant metrics and adequate analysis.
 
-*Robustness testing.* To measure output stability and, by proxy, LM reliability, running inference across paraphrased inputs, synonym substitutes, and varied prompt structures may surface inconsistent strategic reasoning @nalbandyan_score_2025. Testing both surface-level, syntactic robustness and semantic equivalence can largely be automated through use of auxiliary and smaller LMs, and integrated into deployed workflows to inform user confidence in outputs.
+*Robustness testing.* To measure output stability and, by proxy, LM reliability, running inference across paraphrased inputs, synonym substitutes, and varied prompt structures may surface inconsistent strategic reasoning @nalbandyan_score_2025 @shrivastava_inconsistency_2024. Testing both surface-level, syntactic robustness and semantic equivalence can largely be automated through use of auxiliary and smaller LMs, and integrated into deployed workflows to inform user confidence in outputs.
 
 *Calibration assessment.* Well-calibrated models, those which are correct as much as their expressed confidence predicts, help minimize overconfidence in flawed strategic assessments or under-confidence in sound reasoning, providing an important auditing mechanism for understanding LM decisions; measurements of LM calibration allow external stakeholders of wargames to understand systematic flaws in LM decision-making. Additionally, requiring LMs to quantify uncertainty is likely to improve agent performance and make human review of key actions more efficient, particularly in high-stakes situations @liu_uncertainty_2025. 
 
 *Evaluation awareness monitoring.* LMs have been shown to reliably be aware when they are in evaluation contexts @needham_large_2025, and may perform differently when aware they are being tested @abdelnabi_linear_2025, potentially masking real-world failure modes, leading to spurious errors, or displaying deceptive sophisticated reasoning or output during assessment phases. This is of particular concern with wargaming applications because recursive simulations may further distort results. Practitioners should measure evaluation awareness both through motivated questioning during scenarios (e.g., "Do you believe you, as an AI model, are being evaluated?") and passive Chain-of-thought (CoT) monitoring, and episodes containing clear evidence of evaluation awareness should be reevaluated. 

diff --git a/content/07_safety.typ b/content/07_safety.typ
index 9de1244..6ac3b21 100644
--- a/content/07_safety.typ
+++ b/content/07_safety.typ
@@ -9,7 +9,7 @@
-Prior work notes recurring issues when LMs are used for simulation, including bias, (lack of) diversity, and sycophancy @anthis_llm_2025. These observations vary by model and motivate context-dependent guardrails rather than categorical claims about capability. We outline a non-exhaustive set of vulnerabilities as they apply to the most common applications of wargaming, but practitioners should carefully evaluate LM behavior in their own contexts.  
+Prior work notes recurring issues when LMs are used for simulation, including bias, (lack of) diversity, and sycophancy @anthis_llm_2025 @lamparth_human_2024. These observations vary by model and motivate context-dependent guardrails rather than categorical claims about capability. We outline a non-exhaustive set of vulnerabilities as they apply to the most common applications of wargaming, but practitioners should carefully evaluate LM behavior in their own contexts.  
 - *Escalation dynamics:* LMs have shown escalatory tendencies in diplomatic and military contexts @rivera_escalation_2024. Despite proposed mitigation techniques @elbaum_managing_2025, this is an important consideration for military and diplomatic wargaming use-cases.
 - *Implicit bias:* Noise and spurious correlations in pre-training and post-training mean that LMs exhibit social biases despite prompting @taubenfeld_systematic_2024.  Furthermore, @mazeika_UtilityEngineeringAnalyzing_2025 demonstrates that these biases can lead to implicit preferences for certain world states. Bias may lead to systematic blind spots in LMs agents, which is particularly concerning for adversarial modeling.    
 - *Unfaithful reasoning:* @turpin_LanguageModelsDont_2023 @lanham_measuring_2023 show that LMs are not always 'faithful' in their reasoning, meaning their Chain-of-Thought (CoT) may not accurately describe how they arrived at their outputs; in the context of wargaming, this could manifest as misattribution of their decision factors.
 - *Sycophancy:* Off-the-self LMs are trained to mimic assistants, but users prefer agreeable assistants, so a notable artifact of post-training is emergent sycophancy @sharma_UnderstandingSycophancyLanguage_2024. In red-team or human-AI exercises, sycophancy may mask strategic vulnerabilities and incorrectly validate operator assumptions. 

diff --git a/content/06_discussion.typ b/content/06_discussion.typ
index 65bf27a..3d2c9a0 100644
--- a/content/06_discussion.typ
+++ b/content/06_discussion.typ
@@ -41,7 +41,7 @@
-Finance, too, is inherently adversarial and strategic: firms compete for market share, governments set monetary policy, and traders react to limited information. Wargames replicate these dynamics through embedded markets, trade routes, and investment decisions, where each choice has lasting consequences. In Civilization IV, for example, controlling strategic resources functions like commodity market dominance, while trade agreements resemble bilateral contracts. Deploying AI agents in such settings allows economics and finance researchers to observe strategies of collusion, innovation, and predatory expansion under dynamic conditions. This creates an experimental platform for studying algorithmic pricing, market shocks, and competitive equilibria without real-world risk.
+Finance, too, is inherently adversarial and strategic: firms compete for market share, governments set monetary policy, and traders react to limited information. Wargames replicate these dynamics through embedded markets, trade routes, and investment decisions, where each choice has lasting consequences. In Civilization IV, for example, controlling strategic resources functions like commodity market dominance, while trade agreements resemble bilateral contracts. Deploying AI agents in such settings allows economics and finance researchers to observe strategies of collusion, innovation, and predatory expansion under dynamic conditions @hammond_multi_agent_2025. This creates an experimental platform for studying algorithmic pricing, market shocks, and competitive equilibria without real-world risk.

diff --git a/content/01_introduction.typ b/content/01_introduction.typ
index 2f4a1bc..9d8c7e3 100644
--- a/content/01_introduction.typ
+++ b/content/01_introduction.typ
@@ -14,7 +14,7 @@
-The reason for this interest is that traditional computational approaches to wargames often struggle with narrative understanding, social human factors, or unbounded problems @geist_Wargames_2022. The previous limitations on computer-driven wargames meant these wargames relied entirely on SMEs to conduct purely human exercises, which were resource‑intensive and difficult to analyze at scale @mood_WarGamingTechnique_1954 @davis_illustrating_2017 @perla_war_1987. This is the primary reason why advancements in LMs are opening up numerous new possibilities. At the same time, these opportunities pose a serious safety risk during deployment due to the greater degree of freedom afforded by the open-ended nature of these games. Early experiments that let off‑the‑shelf
+The reason for this interest is that traditional computational approaches to wargames often struggle with narrative understanding, social human factors, or unbounded problems @geist_Wargames_2022. The previous limitations on computer-driven wargames meant these wargames relied entirely on SMEs to conduct purely human exercises, which were resource‑intensive and difficult to analyze at scale @mood_WarGamingTechnique_1954 @davis_illustrating_2017 @perla_war_1987. This is the primary reason why advancements in LMs are opening up numerous new possibilities. At the same time, these opportunities pose a serious safety risk during deployment due to the greater degree of freedom afforded by the open-ended nature of these games @rivera_escalation_2024. Early experiments that let off‑the‑shelf
