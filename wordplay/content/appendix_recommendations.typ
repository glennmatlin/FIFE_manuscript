#import "@preview/tracl:0.6.1": *

= Detailed Recommendations <sec:detailedrecommendations>

*Task-specific baselines comparison.* Establishing control conditions using deterministic agents or human SME players or adjudicators enables qualitative and quantitative measurements of LM agent performance in various conditions, and can help detect systematic biases or failure modes unique to LM reasoning @yin_WGSRBenchWargamebasedGametheoretic_2025. Existing human baselines in relevant task spaces (e.g. creative writing, strategic deception) are largely neither transparent nor rigorous enough to provide meaningful comparisons @wei_position_2025. High-stakes wargames therefore should prioritize bespoke evaluations with scenario-relevant metrics and adequate analysis to identify capability gaps and boundary conditions before operational use @lin-greenberg_wargame_2022 @caballero_LargeLanguageModels_2025 @chu_domaino1s_2025 @tang_dsgbench_2025.

*Robustness testing.* To measure LM reliability, running inference across paraphrased inputs, synonym substitutes, and varied prompt structures may surface inconsistent strategic reasoning @shrivastava_measuring_2024 @nalbandyan_SCORESystematicCOnsistency_2025. Testing both surface-level, syntactic robustness and semantic equivalence can largely be automated through use of auxiliary and smaller LMs, and integrated into deployed workflows to inform user confidence in outputs.

*Calibration assessment.* Models with well-calibrated confidence help avoid overreliance on flawed strategic assessments and under-reliance on sound reasoning, providing an important auditing mechanism for understanding LM decisions; measurements of LM calibration allow external stakeholders of wargames to understand systematic flaws in LM decision-making. Additionally, requiring LMs to quantify uncertainty is likely to improve agent performance and make human review of key actions more efficient, particularly in high-stakes situations @liu_UncertaintyQuantificationConfidence_2025 @downes-martin_preference_2020 @freeman_ArtificialIntelligenceWargaming_2024. 

*Validation robustness.* LMs reliably detect evaluation contexts and may perform differently when aware they are being tested @needham_large_2025 @abdelnabi_linear_2025, potentially masking real-world failure modes or displaying deceptive reasoning during assessment. Multiple model architectures should be tested on identical scenarios to identify points of high uncertainty and common failure modes, while evaluation awareness should be monitored through motivated questioning ("Do you believe you are being evaluated?") and passive CoT analysis to improve performance. For instance, cross-model critique, while underperforming external feedback @gou2024critic, outperforms self-correction in multi-agent settings @saleh_evaluating_2025. Episodes with evaluation awareness should be reevaluated, and significant consensus breakdowns may signal events requiring human oversight.

*Human stakeholder training.* LMs' non-intuitive failure modes do not align with the expectation of stakeholders, who are likely to ascribe moral intent to LM output and unlikely to question plain statements @sharma_whywouldyou_2024. Operators need technical understanding of when to trust, how to improve, and where to audit LM outputs. Key stakeholders, including decision-makers relying on LM-enabled wargames, should conceptually understand LM behavioral markers and be provided with confidence assessments of wargame conclusions @ehsan_human-centered_2020.
*Parameter-efficient fine-tuning.* Techniques such as adapters and LoRA, combined with in-context learning (ICL), enable model tailoring for a wargaming context without costly retraining. Lightweight adapters stabilize role-specific behavior (e.g., adjudicator versus player), while prompt-only ICL provides scenario-specific styling and constraints. Both approaches reduce operational overhead and improve reproducibility.