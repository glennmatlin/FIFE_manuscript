#import "../config.typ":num_papers

= Open Research Areas
We highlight priorities in open research questions for integration of LM wargaming agents: 
- *Evaluation Methods:* There are no standard evaluation methods or protocols that measure agent ability over long tasks, interleaving human interactions @reddie_next-generation_2018 @downes-martin_WargamingSupportForce_2025 @reddie_WargamesDataAddressing_2023. Additionally, because using LMs as judges introduces systematic errors @li_llms-as-judges_2024, it is difficult to construct benchmarks without expensive SME trials @wei_position_2025.
- *Long-horizon planning and world models:* Player agents require sophisticated planning algorithms to generate and evaluate alternative long-horizon strategies matching realistic teammates and adversaries. World models predicting move effects enable principled plan rollouts. While common in reinforcement learning, integration with LM's externalized reasoning remains poorly understood. 
- *Robustness to distributional shifts:* Brittle wargaming agent behavior in out-of-distribution (OOD) scenarios can cause failures in novel environments @balloch_NeuroSymbolicWorldModels_2023 @zollicoffer_NoveltyDetectionReinforcement_2025. OOD detection methods @liang_EnhancingReliabilityOutofdistribution_2018 require validation in multi-agent transformer settings with latent distributional shifts @smith_UnderstandingDistributionShift_2024.
- *Persona modeling:* Reliable elicitation and maintenance of coherent hierarchical personas without behavioral leakage between levels remains challenging. Activation steering shows promise @chalnevImprovingSteeringVectors2024 @chen_PersonaVectorsMonitoring_2025 but remains brittle and unpredictable @tanAnalyzingGeneralizationReliability2025 @hao_PatternsMechanismsContrastive_2025.
- *Faithful Interpretability*: Chain-of-Thought (CoT) often proves unfaithful @turpin_LanguageModelsDont_2023, and current interpretability methods cannot recover explanations for tactical decisions in multi-agent wargaming involving deception and opponent modeling. Sparse auto-encoders @huben_SparseAutoencodersFind_2024 and activation patching @ravindran_AdversarialActivationPatching_2025 offer possible ways forward. When it comes to personas, we may wish to know what training sources have influenced the personas and whether the LM's concept of disparate personas are appropriately disentangled.
- *Human-AI (HAI) vs. AI-AI (A2A).* AI in wargames can function either collaboratively or competitively with humans (HAI) or solely against other AI systems (A2A). These roles highlight different AI capabilities; HAI emphasizes human parity and cooperation, while A2A reduces the emphasis on human-like behaviors. Certain quadrant-role pairings, such as Q-IV/HAI, are notably underexplored, and existing literature frequently overlooks explicit differentiation. Negotiation-focused and creatively adaptive agents predominantly align with HAI roles, emphasizing imitation or integration of human diplomatic behaviors. Typically, a single AI agent interacts within groups of human players, particularly in negotiation-centric board games. These interactions commonly occur in random online matches featuring text-based communication, often without human participants being explicitly informed about AI involvement @meta_fundamental_ai_research_diplomacy_team_human-level_2022. Evaluation commonly prioritizes human-like diplomatic interaction and performance relative to human standards.
  However, the importance of A2A contexts grows with the increasing deployment of agentic AI in organizational and enterprise environments. Conflicts involving multiple autonomous agents inherently differ from human-involved dynamics. While existing studies explore cooperative tasks or model coordination, work on competitive diplomatic interactions among AI agents is underexplored. Consequently, human-likeness, a standard benchmark in HAI, may not adequately transfer to A2A settings characterized by distinct equilibria and strategic considerations.