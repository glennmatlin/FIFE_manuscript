#import "../config.typ":num_papers

= Open Research Areas
We highlight priorities in open research questions for integration of LM wargaming agents: 
- *Evaluation Methods:* There are no standard evaluation methods or protocols that measure agent ability over long tasks, interleaving human interactions @reddie_next-generation_2018 @downes-martin_WargamingSupportForce_2025 @reddie_WargamesDataAddressing_2023. Additionally, because using LMs as judges introduces systematic errors @li_llms-as-judges_2024, it is difficult to construct benchmarks without expensive SME trials @wei_position_2025.
- *Long-horizon planning and world models:* Player agents require sophisticated planning algorithms to generate and evaluate alternative long-horizon strategies matching realistic teammates and adversaries. World models predicting move effects enable principled plan rollouts. While common in reinforcement learning, integration with LM's externalized reasoning remains poorly understood. 
- *Robustness to distributional shifts:* Brittle wargaming agent behavior in out-of-distribution (OOD) scenarios can cause failures in novel environments @balloch_NeuroSymbolicWorldModels_2023 @zollicoffer_NoveltyDetectionReinforcement_2025. OOD detection methods @liang_EnhancingReliabilityOutofdistribution_2018 require validation in multi-agent transformer settings with latent distributional shifts @smith_UnderstandingDistributionShift_2024.
- *Persona modeling:* Reliable elicitation and maintenance of coherent hierarchical personas without behavioral leakage between levels remains challenging. Activation steering shows promise @chalnevImprovingSteeringVectors2024 @chen_PersonaVectorsMonitoring_2025 but remains brittle and unpredictable @tanAnalyzingGeneralizationReliability2025 @hao_PatternsMechanismsContrastive_2025.
- *Faithful Interpretability*: Chain-of-thought (CoT) often proves unfaithful @turpin_LanguageModelsDont_2023, and current interpretability methods cannot recover explanations for tactical decisions in multi-agent wargaming involving deception and opponent modeling. Sparse auto-encoders @huben_SparseAutoencodersFind_2024 and activation patching @ravindran_AdversarialActivationPatching_2025 offer possible ways forward. When it comes to personas, we may wish to know what training sources have influenced the personas and whether the LM's concept of disparate personas are appropriately disentangled.
- *Human-AI (HAI) vs AI-AI (A2A)*: AI in wargames can manifest either as a consultant to humans (HAI), a player against humans (HAI) or against AI (A2A). Different roles emphasize different AI capabilities with HAI focusing on human parity and collaboration while A2A deemphasizes human-like performance. Certain quadrant-role pairings are underrepresented (e.g. Q4/HAI), and existing work often fail to distinguish the feature. 
// Additional details and additional open research questions are provided in @app:future-work-details.