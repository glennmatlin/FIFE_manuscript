{"key":"peng_guiding_2022","at_key":"@peng_guiding_2022","title":"Guiding Neural Story Generation with Reader Models (StoRM)","year":2022,"authors":["Peng, Xiangyu","Xie, Kaige","Alabdulkarim, Amal","Kayam, Harshith","Dani, Samihan","Riedl, Mark O."],"venue":"Findings of EMNLP 2022","identifiers":{"doi":"10.18653/v1/2022.findings-emnlp.526","arxiv":"2112.08596"},"url":"https://aclanthology.org/2022.findings-emnlp.526/","bluf":"Adds an explicit reader‑model knowledge graph of the story world; selects LM continuations that minimize distance to a target world state, improving coherence and goal adherence.","wargame_relevance":"Blueprint for keeping free‑text play coherent while steering toward scenario goals (e.g., de‑escalation, sanctions enacted).","triggers":["narrative coherence","reader model","knowledge graph","goal state","story world","on-topic","plot plausibility","controllability"],"cite_when":[{"scenario":"Explaining guardrails that keep open-ended LLM role‑play aligned with scenario goals.","template":"… while maintaining narrative coherence and steering toward predefined world states [@peng_guiding_2022]."},{"scenario":"Describing knowledge‑graph control of story/world state during play.","template":"We represent the scenario state as a knowledge graph and select continuations that move toward desired outcomes [@peng_guiding_2022]."}]}
{"key":"alabdulkarim_goal_2021","at_key":"@alabdulkarim_goal_2021","title":"Goal‑Directed Story Generation: Augmenting Generative Language Models with Reinforcement Learning","year":2021,"authors":["Alabdulkarim, Amal","Li, Winston","Martin, Lara J.","Riedl, Mark O."],"venue":"arXiv preprint","identifiers":{"arxiv":"2112.08593","doi":null},"url":"https://arxiv.org/abs/2112.08593","bluf":"Two methods—PPO fine‑tuning and a graph‑policy selector—steer an LM toward a specified goal event without sacrificing overall quality.","wargame_relevance":"Operationalizes goal‑seeking narrative control (e.g., reaching ceasefire by Turn N) while preserving open‑ended language.","triggers":["goal-directed","reinforcement learning","PPO","reward shaping","graph attention","policy selects LM continuation"],"cite_when":[{"scenario":"Claiming RL fine‑tuning can make LLM narrative actions goal‑seeking.","template":"… using RL (e.g., PPO and graph‑policy selection) to bias narrative actions toward target outcomes [@alabdulkarim_goal_2021]."},{"scenario":"Discussing selection among multiple LM candidates via a policy.","template":"… we let a policy network choose among LM candidates to better reach scenario goals [@alabdulkarim_goal_2021]."}]}
{"key":"alabdulkarim_automatic_2021","at_key":"@alabdulkarim_automatic_2021","title":"Automatic Story Generation: Challenges and Attempts","year":2021,"authors":["Alabdulkarim, Amal","Li, Siyan","Peng, Xiangyu"],"venue":"ACL Workshop on Narrative Understanding (NUSE)","identifiers":{"doi":"10.18653/v1/2021.nuse-1.8","arxiv":"2102.12634"},"url":"https://aclanthology.org/2021.nuse-1.8/","bluf":"Survey highlighting core hurdles in ML story generation: controllability, commonsense, character actions, and creative language.","wargame_relevance":"Background justification for why open‑ended LLM wargames need control and coherence mechanisms.","triggers":["survey","controllability","commonsense","character actions","creative language"],"cite_when":[{"scenario":"Motivating the need for controllability/coherence in narrative wargames.","template":"Prior work identifies controllability and coherence as central challenges in story generation [@alabdulkarim_automatic_2021]."},{"scenario":"Framing creative/novelty evaluation in narrative systems.","template":"… creativity and character‑action plausibility remain open issues [@alabdulkarim_automatic_2021]."}]}
{"key":"alabdulkarim_experiential_2022","at_key":"@alabdulkarim_experiential_2022","title":"Experiential Explanations for Reinforcement Learning","year":2022,"authors":["Alabdulkarim, Amal","Singh, Madhuri","Mansi, Gennie","Hall, Kaely","Riedl, Mark O."],"venue":"arXiv preprint (journal version: Neural Computing & Applications, 2025)","identifiers":{"arxiv":"2210.04723","journal_doi":"10.1007/s00521-024-10951-3"},"url":"https://arxiv.org/abs/2210.04723","bluf":"Trains influence‑predictor models alongside the policy to answer counterfactual “why‑not” questions by linking reward sources to behavior.","wargame_relevance":"Useful for AARs and adjudication: tie sequential decisions to incentives/signals during the exercise.","triggers":["counterfactual","why-not","influence predictors","reward sources","sequential decisions"],"cite_when":[{"scenario":"Explaining why an agent took or didn’t take a move under specific incentives.","template":"… providing counterfactual, reward‑linked rationales for sequential decisions [@alabdulkarim_experiential_2022]."},{"scenario":"Designing explainability for multi‑turn RL/agent behavior in play.","template":"We attach influence predictors to trace how signals shaped actions [@alabdulkarim_experiential_2022]."}]}
{"key":"ehsan_automated_2019","at_key":"@ehsan_automated_2019","title":"Automated Rationale Generation: A Technique for Explainable AI and its Effects on Human Perceptions","year":2019,"authors":["Ehsan, Upol","Tambwekar, Pradyumna","Chan, Larry","Harrison, Brent","Riedl, Mark O."],"venue":"IUI 2019","identifiers":{"doi":"10.1145/3301275.3302316","arxiv":"1901.03729"},"url":"https://dl.acm.org/doi/10.1145/3301275.3302316","bluf":"Maps internal state/action traces to natural‑language rationales; human studies show detailed rationales can improve confidence and understanding.","wargame_relevance":"Pattern for move‑by‑move justifications in qualitative wargames to support trust and sense‑making.","triggers":["rationale generation","natural-language explanations","user study","confidence","understanding"],"cite_when":[{"scenario":"Claiming that natural‑language rationales improve human perceptions/trust.","template":"… generate human‑readable rationales for each move to improve user understanding [@ehsan_automated_2019]."},{"scenario":"Describing a pipeline from state/action traces to NL rationales.","template":"We translate action traces into rationales visible to players/umpires [@ehsan_automated_2019]."}]}
{"key":"ehsan_rationalization_2018","at_key":"@ehsan_rationalization_2018","title":"Rationalization: A Neural Machine Translation Approach to Generating Natural‑Language Explanations","year":2018,"authors":["Ehsan, Upol","Harrison, Brent","Chan, Larry","Riedl, Mark O."],"venue":"AIES 2018","identifiers":{"doi":"10.1145/3278721.3278736","arxiv":"1702.07826"},"url":"https://dl.acm.org/doi/10.1145/3278721.3278736","bluf":"Introduces AI rationalization—treating explanations as translation from internal state/action to human‑style narratives.","wargame_relevance":"Foundation for explain‑as‑you‑play narration of AI actors’ intent.","triggers":["AI rationalization","neural machine translation","state-action translation","Frogger"],"cite_when":[{"scenario":"Arguing for narrative explanations even when internal reasoning isn’t directly introspectable.","template":"… treat explanations as translation into human‑style rationales [@ehsan_rationalization_2018]."},{"scenario":"Linking game‑like sequential tasks to rationale generation.","template":"… prior game‑context work shows feasibility of rationale generation [@ehsan_rationalization_2018]."}]}
{"key":"ehsan_learning_2018","at_key":"@ehsan_learning_2018","title":"Learning to Generate Natural‑Language Rationales for Game‑Playing Agents","year":2018,"authors":["Ehsan, Upol","Tambwekar, Pradyumna","Chan, Larry","Harrison, Brent","Riedl, Mark O."],"venue":"AAAI Workshop on Experimental AI in Games (EXAG)","identifiers":{"doi":null,"arxiv":null,"alt_url":"https://uknowledge.uky.edu/cs_facpub/18/"},"url":"https://uknowledge.uky.edu/cs_facpub/18/","bluf":"Data‑collection + neural pipeline for believable in‑game rationales using think‑aloud traces.","wargame_relevance":"Template for collecting domain‑specific rationale data for turn‑based exercises.","triggers":["think‑aloud","rationale dataset","sequential/turn‑based","pipeline"],"cite_when":[{"scenario":"Describing how to collect/curate rationale corpora for your domain.","template":"We adapt a think‑aloud collection pipeline to train rationale generators [@ehsan_learning_2018]."},{"scenario":"Methodological background for training rationale models for games.","template":"… prior EXAG work on rationale pipelines informs our setup [@ehsan_learning_2018]."}]}
{"key":"harrison_guiding_2017","at_key":"@harrison_guiding_2017","title":"Guiding Reinforcement Learning Exploration Using Natural Language","year":2017,"authors":["Harrison, Brent","Ehsan, Upol","Riedl, Mark O."],"venue":"arXiv preprint (AAMAS 2018 extended abstract)","identifiers":{"arxiv":"1707.08616","doi":null},"url":"https://arxiv.org/abs/1707.08616","bluf":"Maps free‑text guidance to state‑action information via NMT and integrates it into policy shaping to improve generalization.","wargame_relevance":"Use doctrine/ROE‑like text to shape agent exploration and behavior policies.","triggers":["policy shaping","natural language guidance","encoder‑decoder","generalization","Frogger"],"cite_when":[{"scenario":"Claiming natural‑language instructions can shape RL behavior.","template":"… incorporate free‑text guidance into exploration via policy shaping [@harrison_guiding_2017]."},{"scenario":"Using human‑authored doctrine/ROE as priors.","template":"We translate doctrine‑style text into guidance for action selection [@harrison_guiding_2017]."}]}
{"key":"ehsan_expanding_2021","at_key":"@ehsan_expanding_2021","title":"Expanding Explainability: Towards Social Transparency in AI Systems","year":2021,"authors":["Ehsan, Upol","Liao, Q. Vera","Muller, Michael","Riedl, Mark O.","Weisz, Justin D."],"venue":"CHI 2021","identifiers":{"doi":"10.1145/3411764.3445188","arxiv":"2101.04719"},"url":"https://dl.acm.org/doi/10.1145/3411764.3445188","bluf":"Introduces Social Transparency (ST): explanations should include socio‑organizational context (who decided, how outputs were used), not just model internals.","wargame_relevance":"Design lens for AARs and governance of AI‑assisted role‑play; calibrates trust and accountability.","triggers":["social transparency","sociotechnical","organizational context","calibrated trust"],"cite_when":[{"scenario":"Arguing that explanations must cover process, actors, and usage—not only models.","template":"We adopt Social Transparency to capture socio‑organizational context [@ehsan_expanding_2021]."},{"scenario":"Specifying explanation policies for different roles (players, umpires, sponsors).","template":"… align explanation targets with stakeholder needs via ST [@ehsan_expanding_2021]."}]}
{"key":"ehsan_charting_2023","at_key":"@ehsan_charting_2023","title":"Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI","year":2023,"authors":["Ehsan, Upol","Saha, Koustuv","De Choudhury, Munmun","Riedl, Mark O."],"venue":"PACM HCI (CSCW) 2023","identifiers":{"doi":"10.1145/3579467","arxiv":"2302.00799"},"url":"https://dl.acm.org/doi/10.1145/3579467","bluf":"Framework and cases for connecting stakeholder needs to XAI affordances; operational guidance to bridge the sociotechnical gap.","wargame_relevance":"Helps define role‑specific explanation requirements and evaluation during/after exercises.","triggers":["sociotechnical gap","framework","stakeholder needs","guidelines"],"cite_when":[{"scenario":"Designing explanation requirements across heterogeneous participants.","template":"… we chart stakeholder needs to XAI affordances [@ehsan_charting_2023]."},{"scenario":"Discussing operational governance of explanations.","template":"… adopt a framework to bridge social needs and technical artifacts [@ehsan_charting_2023]."}]}
{"key":"ehsan_human_centered_2020","at_key":"@ehsan_human_centered_2020","title":"Human‑Centered Explainable AI: Towards a Reflective Sociotechnical Approach","year":2020,"authors":["Ehsan, Upol","Riedl, Mark O."],"venue":"arXiv preprint (HCI Int’l)","identifiers":{"arxiv":"2002.01092","doi":null},"url":"https://arxiv.org/abs/2002.01092","bluf":"Positions HCXAI: a reflective approach that centers values, context, and stakeholders; includes a case study and open questions.","wargame_relevance":"Supports participant‑appropriate explanation UX for qualitative exercises.","triggers":["HCXAI","human‑centered","reflective","value‑sensitive design"],"cite_when":[{"scenario":"Stating that explanation design must start from human needs/contexts.","template":"… we follow a human‑centered, reflective XAI approach [@ehsan_human_centered_2020]."},{"scenario":"Motivating mixed‑method evaluation of explanations.","template":"… evaluate with human factors in mind [@ehsan_human_centered_2020]."}]}
{"key":"lin_beyond_2023","at_key":"@lin_beyond_2023","title":"Beyond Prompts: Exploring the Design Space of Mixed‑Initiative Co‑Creativity Systems","year":2023,"authors":["Lin, Zhiyu","Ehsan, Upol","Agarwal, Rohan","Dani, Samihan","Vashishth, Vidushi","Riedl, Mark O."],"venue":"ICCC 2023 / arXiv","identifiers":{"arxiv":"2305.07465","doi":null},"url":"https://arxiv.org/abs/2305.07465","bluf":"Defines a design space for mixed‑initiative co‑creativity; user study (n=185) shows broader MI‑CC capabilities improve outcomes; highlights scrutability & explainability.","wargame_relevance":"Design cues for human‑AI co‑facilitation of narrative injects, surprises, and control without chaos.","triggers":["mixed‑initiative","co‑creative","scrutability","explainability","user study"],"cite_when":[{"scenario":"Arguing for mixed‑initiative tooling (not just prompting) in wargame facilitation.","template":"… beyond prompts, MI‑CC designs support better outcomes [@lin_beyond_2023]."},{"scenario":"Highlighting scrutability and explainability as desired features.","template":"Participants valued scrutability/explainability in MI‑CC systems [@lin_beyond_2023]."}]}
