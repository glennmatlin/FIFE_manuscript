{"id":"shrivastava_inconsistency_2024","title":"Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations","authors":["Aryan Shrivastava","Jessica Hullman","Max Lamparth"],"year":2024,"bibtex":"@inproceedings{shrivastava_inconsistency_2024, title={Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations}, author={Aryan Shrivastava and Jessica Hullman and Max Lamparth}, booktitle={NeurIPS 2024 Workshop on Socially Responsible Language Modeling Research (SoLaR)}, year={2024}, note={poster; see arXiv:2410.13204}, url={https://openreview.net/forum?id=qZ2CeIaYSu}}","bluf":"Introduces a semantic (BERTScore-based) inconsistency metric for free-form LLM outputs in crisis wargames; finds non-trivial instability across models and prompts and warns of prompt-sensitivity risks for decision support.","keywords":["LLM","inconsistency","wargame","crisis simulation","evaluation"],"top_claims":["LLMs produce measurable semantic inconsistency in free-form wargame answers","Prompt phrasing can induce as much instability as sampling variability"],"url":"https://openreview.net/forum?id=qZ2CeIaYSu"}
{"id":"lamparth_human_vs_machine_2024","title":"Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations","authors":["Max Lamparth","Anthony Corso","Jacob Ganz","Oriana Skylar Mastro","Jacquelyn Schneider","Harold Trinkunas"],"year":2024,"bibtex":"@article{lamparth_human_vs_machine_2024, title={Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations}, author={Max Lamparth and Anthony Corso and Jacob Ganz and Oriana Skylar Mastro and Jacquelyn Schneider and Harold Trinkunas}, journal={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}, year={2024}, doi={10.1609/aies.v7i1.31681}, url={https://ojs.aaai.org/index.php/AIES/article/view/31681}}","bluf":"Compares expert human teams to LLM-simulated teams in the same crisis wargame; finds systematic behavioral divergences (e.g., aggression, lack of intra-team disagreement dynamics) even when high-level agreement exists.","keywords":["human-vs-LLM","wargame","behavioral comparison","simulation"],"top_claims":["LLM teams diverge from experts on certain action types and dynamics","LLMs often lack realistic disagreement/dialogue structure seen in human teams"],"url":"https://ojs.aaai.org/index.php/AIES/article/view/31681"}
{"id":"rivera_escalation_2024","title":"Escalation Risks from Language Models in Military and Diplomatic Decision-Making","authors":["Juan-Pablo Rivera","Gabriel Mukobi","Anka Reuel","Max Lamparth","Chandler Smith","Jacquelyn Schneider"],"year":2024,"bibtex":"@inproceedings{rivera_escalation_2024, title={Escalation Risks from Language Models in Military and Diplomatic Decision-Making}, author={Juan-Pablo Rivera and Gabriel Mukobi and Anka Reuel and Max Lamparth and Chandler Smith and Jacquelyn Schneider}, booktitle={Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)}, year={2024}, doi={10.1145/3630106.3658942}, url={https://dl.acm.org/doi/10.1145/3630106.3658942}}","bluf":"Multi-agent simulations using off-the-shelf LLMs can generate hard-to-predict escalation dynamics (including arms-race logic and rare high-consequence outcomes); offers an escalation-scoring framework to quantify risk.","keywords":["escalation","multi-agent","LLM","wargame","risk"],"top_claims":["LLM-driven agents can produce destabilizing arms-race dynamics","Quantitative escalation scores help compare policies and prompts"],"url":"https://dl.acm.org/doi/10.1145/3630106.3658942"}
{"id":"hammond_multi_agent_2025","title":"Multi-Agent Risks from Advanced AI (Technical Report #1)","authors":["Lewis Hammond","Alan Chan","Jesse Clifton","Jason Hoelscher-Obermaier","Akbir Khan","Euan McLean","Chandler Smith","Wolfram Barfuss","Jakob Foerster","Tomáš Gavenčiak","The Anh Han","Edward Hughes","Vojtěch Kovařík","Jan Kulveit","Joel Z. Leibo","Caspar Oesterheld","Christian Schroeder de Witt","Nisarg Shah","Michael Wellman","Max Lamparth"],"year":2025,"bibtex":"@techreport{hammond_multi_agent_2025, title={Multi-Agent Risks from Advanced AI}, author={Lewis Hammond and Alan Chan and Jesse Clifton and Jason Hoelscher-Obermaier and Akbir Khan and Euan McLean and Chandler Smith and Wolfram Barfuss and Jakob Foerster and Tomáš Gavenčiak and The Anh Han and Edward Hughes and Vojtěch Kovařík and Jan Kulveit and Joel Z. Leibo and Caspar Oesterheld and Christian Schroeder de Witt and Nisarg Shah and Michael Wellman and Max Lamparth and others}, institution={Cooperative AI Foundation}, year={2025}, note={Technical Report #1; arXiv:2502.14143}, url={https://arxiv.org/abs/2502.14143}}","bluf":"Taxonomy and analysis of multi-agent failure modes (miscoordination, collusion, conflict) plus evaluation & mitigation recommendations—encourages open-ended, multi-agent wargame evaluations as safety tools.","keywords":["multi-agent","risks","cooperative AI","wargame evaluation","taxonomy"],"top_claims":["Catalogs multi-agent failure modes likely to emerge with advanced AI","Recommends open-ended multi-agent simulations (wargame-style) for risk assessment"],"url":"https://arxiv.org/abs/2502.14143"}
{"id":"huang_adaptive_hhh_2025","title":"We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles","authors":["Yizheng Huang","Chenyang Gao","Y. Zhou","K. Guo","Y. Lin","X. Wang","Or Cohen-Sasson","Max Lamparth","X. Zhang"],"year":2025,"bibtex":"@article{huang_adaptive_hhh_2025, title={We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles}, author={Yizheng Huang and Chenyang Gao and Y. Zhou and K. Guo and Y. Lin and X. Wang and Or Cohen-Sasson and Max Lamparth and X. Zhang}, journal={arXiv preprint arXiv:2502.06059}, year={2025}, url={https://arxiv.org/abs/2502.06059}}","bluf":"Argues H/H/H alignment principles should be applied adaptively by context and capability, proposing interpretive guidance and evaluation suggestions for nuanced alignment across domains.","keywords":["alignment","safety","helpful honest harmless","policy"],"top_claims":["H/H/H principles need context-sensitive interpretation","Proposes evaluation heuristics for adaptive alignment"],"url":"https://arxiv.org/abs/2502.06059"}
{"id":"huang_trustworthiness_2025","title":"On the Trustworthiness of Generative Foundation Models — Guideline, Assessment, and Perspective","authors":["Yizheng Huang","Chenyang Gao","S. Wu","H. Wang","Max Lamparth","X. Zhang","and others"],"year":2025,"bibtex":"@article{huang_trustworthiness_2025, title={On the Trustworthiness of Generative Foundation Models -- Guideline, Assessment, and Perspective}, author={Yizheng Huang and Chenyang Gao and S. Wu and H. Wang and Max Lamparth and X. Zhang and others}, journal={arXiv preprint arXiv:2502.14296}, year={2025}, url={https://arxiv.org/abs/2502.14296}}","bluf":"Survey and practical guideline for assessing trustworthiness of generative foundation models; synthesizes axes of evaluation and governance practices.","keywords":["trustworthiness","foundation models","evaluation","governance"],"top_claims":["Presents a unified evaluation framework for generative models","Provides actionable guidance for trust & governance assessments"],"url":"https://arxiv.org/abs/2502.14296"}
{"id":"reuel_betterbench_2024","title":"BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices","authors":["Anka Reuel","Abel Hardy","Chandler Smith","Max Lamparth","Michael Hardy","Mykel Kochenderfer"],"year":2024,"bibtex":"@article{reuel_betterbench_2024, title={BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices}, author={Anka Reuel and Abel Hardy and Chandler Smith and Max Lamparth and Michael Hardy and Mykel Kochenderfer}, journal={arXiv preprint arXiv:2411.12990}, year={2024}, note={NeurIPS 2024 Datasets & Benchmarks Track (Spotlight)}, url={https://arxiv.org/abs/2411.12990}}","bluf":"Audits modern AI benchmarks, identifies common failure modes (leakage, overfitting, brittle scoring), and prescribes best practices to design robust, meaningful benchmarks.","keywords":["benchmarks","evaluation","best-practices","benchmark design"],"top_claims":["Many widely-used AI benchmarks suffer avoidable design flaws","Prescribes practical fixes to reduce brittleness and gaming"],"url":"https://arxiv.org/abs/2411.12990"}
{"id":"lamparth_backdoors_2024","title":"Analyzing and Editing Inner Mechanisms of Backdoored Language Models","authors":["Max Lamparth","Anka Reuel"],"year":2024,"bibtex":"@inproceedings{lamparth_backdoors_2024, title={Analyzing And Editing Inner Mechanisms of Backdoored Language Models}, author={Max Lamparth and Anka Reuel}, booktitle={Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)}, year={2024}, doi={10.1145/3630106.3659042}, note={see arXiv preprint arXiv:2302.12461}, url={https://dl.acm.org/doi/10.1145/3630106.3659042}}","bluf":"Investigates how backdoors are implemented inside transformer models, localizes mechanisms (notably early MLPs), and demonstrates an editing/ablation technique to remove or mitigate backdoors.","keywords":["backdoor","security","model editing","transformer"],"top_claims":["Backdoors localize to early model components in many cases","Targeted editing (PCP ablation) can remove backdoor behavior"],"url":"https://dl.acm.org/doi/10.1145/3630106.3659042"}
{"id":"viteri_markovian_2024","title":"Markovian Agents for Truthful Language Modeling","authors":["Santiago Viteri","Max Lamparth","Pierre Chatain","Clark Barrett"],"year":2024,"bibtex":"@article{viteri_markovian_2024, title={Markovian Agents for Truthful Language Modeling}, author={Santiago Viteri and Max Lamparth and Pierre Chatain and Clark Barrett}, journal={arXiv preprint arXiv:2404.18988}, year={2024}, url={https://arxiv.org/abs/2404.18988}}","bluf":"Proposes a Markovian agent structure for language-model agents to improve stepwise reasoning and reduce hallucination, with implications for truthful, sequential decision-making.","keywords":["truthfulness","agent structure","Markovian","LLM"],"top_claims":["Enforcing Markovian structure reduces certain hallucinations in sequential decisions","Markovian agents facilitate stepwise reasoning useful for decision tasks"],"url":"https://arxiv.org/abs/2404.18988"}
{"id":"phan_humanitys_last_exam_2025","title":"Humanity's Last Exam","authors":["Long Phan","and others (benchmark contributors including Max Lamparth)"],"year":2025,"bibtex":"@article{phan_humanitys_last_exam_2025, title={Humanity's Last Exam}, author={Long Phan and others}, journal={arXiv preprint arXiv:2501.14249}, year={2025}, url={https://arxiv.org/abs/2501.14249}, note={Benchmark contributor}}","bluf":"A large benchmark suite intended to probe broad, human-level capabilities of foundation models across many axes; useful as a general-capability baseline when evaluating specialized agents.","keywords":["benchmark","capabilities","evaluation","foundation models"],"top_claims":["Broad-suite benchmark exposes strengths/weaknesses of foundation models","Useful as baseline comparison for specialized task evaluations"],"url":"https://arxiv.org/abs/2501.14249"}
{"id":"lamparth_medical_ambiguity_2025","title":"Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare","authors":["Max Lamparth","Daniel Grabb","Alistair Franks","S. Gershan","K. N. Kunstman","A. Lulla","M. D. Roots","M. Sharma","A. Shrivastava","N. Vasan","C. Waickman"],"year":2025,"bibtex":"@article{lamparth_medical_ambiguity_2025, title={Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare}, author={Max Lamparth and Daniel Grabb and Alistair Franks and S. Gershan and K. N. Kunstman and A. Lulla and M. D. Roots and M. Sharma and A. Shrivastava and N. Vasan and C. Waickman}, journal={arXiv preprint arXiv:2502.16051}, year={2025}, url={https://arxiv.org/abs/2502.16051}}","bluf":"Presents a clinician-annotated dataset capturing real-world task ambiguity in mental healthcare, highlighting evaluation challenges beyond exam-style QA and demonstrating the need for richer evaluation protocols.","keywords":["dataset","ambiguity","evaluation","mental healthcare"],"top_claims":["Real-world tasks present ambiguity not captured by exam QA datasets","Clinician annotations enable more realistic evaluation of model helpfulness"],"url":"https://arxiv.org/abs/2502.16051"}
{"id":"grabb_mentalhealth_2024","title":"Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation","authors":["Daniel Grabb","Max Lamparth","Neeraj Vasan"],"year":2024,"bibtex":"@article{grabb_mentalhealth_2024, title={Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation}, author={Daniel Grabb and Max Lamparth and Neeraj Vasan}, journal={arXiv preprint arXiv:2406.11852}, year={2024}, note={CoLM 2024}, url={https://arxiv.org/abs/2406.11852}}","bluf":"Surveys ethical and structural risks of deploying LLMs in mental-health contexts and proposes an implementation framework with guardrails to mitigate harm in sensitive, high-stakes deployments.","keywords":["ethics","mental healthcare","risks","deployment"],"top_claims":["LLMs pose special risks in automated mental health delivery that require structured governance","Proposes layered mitigations and evaluation structures for safe deployment"],"url":"https://arxiv.org/abs/2406.11852"}
