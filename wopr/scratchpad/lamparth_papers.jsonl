{"id":"lamparth_2024_inconsistency","title":"Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations","authors":["Aryan Shrivastava","Jessica Hullman","Max Lamparth"],"year":2024,"bibtex":"@inproceedings{shrivastava_hullman_lamparth_2024_inconsistency, title={Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations}, author={Shrivastava, Aryan and Hullman, Jessica and Lamparth, Max}, booktitle={NeurIPS 2024 Workshop on Socially Responsible Language Modeling Research (SoLaR)}, year={2024}, note={poster; see arXiv:2410.13204}, url={https://openreview.net/forum?id=qZ2CeIaYSu}}","bluf":"Introduces a semantic (BERTScore-based) inconsistency metric for free-form LLM outputs in crisis wargames; finds non-trivial instability across models and prompts and warns of prompt-sensitivity risks for decision support.","keywords":["LLM","inconsistency","wargame","crisis simulation","evaluation"],"top_claims":["LLMs produce measurable semantic inconsistency in free-form wargame answers","Prompt phrasing can induce as much instability as sampling variability"],"url":"https://openreview.net/forum?id=qZ2CeIaYSu"}
{"id":"lamparth_2024_human_vs_machine","title":"Human vs. Machine: Behavioral Differences Between Expert Humans and Language Models in Wargame Simulations","authors":["Max Lamparth","Anthony Corso","Jacob Ganz","Oriana Skylar Mastro","Jacquelyn Schneider","Harold Trinkunas"],"year":2024,"bibtex":"@article{lamparth_et_al_2024_human_vs_machine, title={Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations}, author={Lamparth, Max and Corso, Anthony and Ganz, Jacob and Mastro, Oriana Skylar and Schneider, Jacquelyn and Trinkunas, Harold}, journal={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society}, year={2024}, doi={10.1609/aies.v7i1.31681}, url={https://ojs.aaai.org/index.php/AIES/article/view/31681}}","bluf":"Compares expert human teams to LLM-simulated teams in the same crisis wargame; finds systematic behavioral divergences (e.g., aggression, lack of intra-team disagreement dynamics) even when high-level agreement exists.","keywords":["human-vs-LLM","wargame","behavioral comparison","simulation"],"top_claims":["LLM teams diverge from experts on certain action types and dynamics","LLMs often lack realistic disagreement/dialogue structure seen in human teams"],"url":"https://ojs.aaai.org/index.php/AIES/article/view/31681"}
{"id":"rivera_2024_escalation_risks","title":"Escalation Risks from Language Models in Military and Diplomatic Decision-Making","authors":["Juan-Pablo Rivera","Gabriel Mukobi","Anka Reuel","Max Lamparth","Chandler Smith","Jacquelyn Schneider"],"year":2024,"bibtex":"@inproceedings{rivera_et_al_2024_escalation, title={Escalation Risks from Language Models in Military and Diplomatic Decision-Making}, author={Rivera, Juan-Pablo and Mukobi, Gabriel and Reuel, Anka and Lamparth, Max and Smith, Chandler and Schneider, Jacquelyn}, booktitle={Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)}, year={2024}, doi={10.1145/3630106.3658942}, url={https://dl.acm.org/doi/10.1145/3630106.3658942}}","bluf":"Multi-agent simulations using off-the-shelf LLMs can generate hard-to-predict escalation dynamics (including arms-race logic and rare high-consequence outcomes); offers an escalation-scoring framework to quantify risk.","keywords":["escalation","multi-agent","LLM","wargame","risk"],"top_claims":["LLM-driven agents can produce destabilizing arms-race dynamics","Quantitative escalation scores help compare policies and prompts"],"url":"https://dl.acm.org/doi/10.1145/3630106.3658942"}
{"id":"cooperativeai_2025_multiagent_risks","title":"Multi-Agent Risks from Advanced AI (Technical Report #1)","authors":["Lewis Hammond","Alan Chan","Jesse Clifton","Jason Hoelscher-Obermaier","Akbir Khan","Euan McLean","Chandler Smith","Wolfram Barfuss","Jakob Foerster","Tomáš Gavenčiak","The Anh Han","Edward Hughes","Vojtěch Kovařík","Jan Kulveit","Joel Z. Leibo","Caspar Oesterheld","Christian Schroeder de Witt","Nisarg Shah","Michael Wellman","Max Lamparth","and others"],"year":2025,"bibtex":"@techreport{cooperativeai_2025_multiagent_risks, title={Multi-Agent Risks from Advanced AI}, author={Hammond, Lewis and Chan, Alan and Clifton, Jesse and Hoelscher-Obermaier, Jason and Khan, Akbir and McLean, Euan and Smith, Chandler and Barfuss, Wolfram and Foerster, Jakob and Gavenčiak, Tom\'a\v{s} and Han, The Anh and Hughes, Edward and Kova\v{r}\'ik, Vojt\v{e}ch and Kulveit, Jan and Leibo, Joel Z. and Oesterheld, Caspar and Schroeder de Witt, Christian and Shah, Nisarg and Wellman, Michael and Lamparth, Max and others}, institution={Cooperative AI Foundation}, year={2025}, note={Technical Report #1; arXiv:2502.14143}, url={https://arxiv.org/abs/2502.14143}}","bluf":"Taxonomy and analysis of multi-agent failure modes (miscoordination, collusion, conflict) plus evaluation & mitigation recommendations—encourages open-ended, multi-agent wargame evaluations as safety tools.","keywords":["multi-agent","risks","cooperative AI","wargame evaluation","taxonomy"],"top_claims":["Catalogs multi-agent failure modes likely to emerge with advanced AI","Recommends open-ended multi-agent simulations (wargame-style) for risk assessment"],"url":"https://arxiv.org/abs/2502.14143"}
{"id":"huang_2025_adaptive_hhh","title":"We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles","authors":["Yizheng Huang","Chenyang Gao","Y. Zhou","K. Guo","Y. Lin","X. Wang","Or Cohen-Sasson","Max Lamparth","X. Zhang"],"year":2025,"bibtex":"@article{huang_et_al_2025_adaptive_hhh, title={We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles}, author={Huang, Yizheng and Gao, Chenyang and Zhou, Y. and Guo, K. and Lin, Y. and Wang, X. and Cohen-Sasson, Or and Lamparth, Max and Zhang, X.}, journal={arXiv preprint arXiv:2502.06059}, year={2025}, url={https://arxiv.org/abs/2502.06059}}","bluf":"Argues H/H/H alignment principles should be applied adaptively by context and capability, proposing interpretive guidance and evaluation suggestions for nuanced alignment across domains.","keywords":["alignment","safety","helpful honest harmless","policy"],"top_claims":["H/H/H principles need context-sensitive interpretation","Proposes evaluation heuristics for adaptive alignment"],"url":"https://arxiv.org/abs/2502.06059"}
{"id":"huang_2025_trustworthiness_gfm","title":"On the Trustworthiness of Generative Foundation Models — Guideline, Assessment, and Perspective","authors":["Yizheng Huang","Chenyang Gao","S. Wu","H. Wang","Max Lamparth","X. Zhang","and others"],"year":2025,"bibtex":"@article{huang_et_al_2025_trustworthiness_gfm, title={On the Trustworthiness of Generative Foundation Models -- Guideline, Assessment, and Perspective}, author={Huang, Yizheng and Gao, Chenyang and Wu, S. and Wang, H. and Lamparth, Max and Zhang, X. and others}, journal={arXiv preprint arXiv:2502.14296}, year={2025}, url={https://arxiv.org/abs/2502.14296}}","bluf":"Survey and practical guideline for assessing trustworthiness of generative foundation models; synthesizes axes of evaluation and governance practices.","keywords":["trustworthiness","foundation models","evaluation","governance"],"top_claims":["Presents a unified evaluation framework for generative models","Provides actionable guidance for trust & governance assessments"],"url":"https://arxiv.org/abs/2502.14296"}
{"id":"reuel_2024_betterbench","title":"BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices","authors":["Anka Reuel","Abel Hardy","Chandler Smith","Max Lamparth","Michael Hardy","Mykel Kochenderfer"],"year":2024,"bibtex":"@article{reuel_et_al_2024_betterbench, title={BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices}, author={Reuel, Anka and Hardy, Abel and Smith, Chandler and Lamparth, Max and Hardy, Michael and Kochenderfer, Mykel}, journal={arXiv preprint arXiv:2411.12990}, year={2024}, note={NeurIPS 2024 Datasets & Benchmarks Track (Spotlight)}, url={https://arxiv.org/abs/2411.12990}}","bluf":"Audits modern AI benchmarks, identifies common failure modes (leakage, overfitting, brittle scoring), and prescribes best practices to design robust, meaningful benchmarks.","keywords":["benchmarks","evaluation","best-practices","benchmark design"],"top_claims":["Many widely-used AI benchmarks suffer avoidable design flaws","Prescribes practical fixes to reduce brittleness and gaming"],"url":"https://arxiv.org/abs/2411.12990"}
{"id":"lamparth_reuel_2024_backdoors","title":"Analyzing and Editing Inner Mechanisms of Backdoored Language Models","authors":["Max Lamparth","Anka Reuel"],"year":2024,"bibtex":"@inproceedings{lamparth_reuel_2024_backdoors, title={Analyzing And Editing Inner Mechanisms of Backdoored Language Models}, author={Lamparth, Max and Reuel, Anka}, booktitle={Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)}, year={2024}, doi={10.1145/3630106.3659042}, note={see arXiv preprint arXiv:2302.12461}, url={https://dl.acm.org/doi/10.1145/3630106.3659042}}","bluf":"Investigates how backdoors are implemented inside transformer models, localizes mechanisms (notably early MLPs), and demonstrates an editing/ablation technique to remove or mitigate backdoors.","keywords":["backdoor","security","model editing","transformer"],"top_claims":["Backdoors localize to early model components in many cases","Targeted editing (PCP ablation) can remove backdoor behavior"],"url":"https://dl.acm.org/doi/10.1145/3630106.3659042"}
{"id":"viteri_2024_markovian_agents","title":"Markovian Agents for Truthful Language Modeling","authors":["Santiago Viteri","Max Lamparth","Pierre Chatain","Clark Barrett"],"year":2024,"bibtex":"@article{viteri_lamparth_chatain_barrett_2024_markovian, title={Markovian Agents for Truthful Language Modeling}, author={Viteri, Santiago and Lamparth, Max and Chatain, Pierre and Barrett, Clark}, journal={arXiv preprint arXiv:2404.18988}, year={2024}, url={https://arxiv.org/abs/2404.18988}}","bluf":"Proposes a Markovian agent structure for language-model agents to improve stepwise reasoning and reduce hallucination, with implications for truthful, sequential decision-making.","keywords":["truthfulness","agent structure","Markovian","LLM"],"top_claims":["Enforcing Markovian structure reduces certain hallucinations in sequential decisions","Markovian agents facilitate stepwise reasoning useful for decision tasks"],"url":"https://arxiv.org/abs/2404.18988"}
{"id":"phan_2025_humanitys_last_exam","title":"Humanity's Last Exam","authors":["Long Phan","and others (benchmark contributors including Max Lamparth)"],"year":2025,"bibtex":"@article{phan_et_al_2025_hle, title={Humanity's Last Exam}, author={Phan, Long and others}, journal={arXiv preprint arXiv:2501.14249}, year={2025}, note={benchmark; contributors include Max Lamparth}, url={https://arxiv.org/abs/2501.14249}}","bluf":"A large benchmark suite intended to probe broad, human-level capabilities of foundation models across many axes; useful as a general-capability baseline when evaluating specialized agents.","keywords":["benchmark","capabilities","evaluation","foundation models"],"top_claims":["Broad-suite benchmark exposes strengths/weaknesses of foundation models","Useful as baseline comparison for specialized task evaluations"],"url":"https://arxiv.org/abs/2501.14249"}
{"id":"lamparth_2025_medical_ambiguity_dataset","title":"Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare","authors":["Max Lamparth","Daniel Grabb","Alistair Franks","S. Gershan","K. N. Kunstman","A. Lulla","M. D. Roots","M. Sharma","A. Shrivastava","N. Vasan","C. Waickman"],"year":2025,"bibtex":"@article{lamparth_et_al_2025_medical_ambiguity, title={Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare}, author={Lamparth, Max and Grabb, Daniel and Franks, Alistair and Gershan, S. and Kunstman, K. N. and Lulla, A. and Roots, M. D. and Sharma, M. and Shrivastava, A. and Vasan, N. and Waickman, C.}, journal={arXiv preprint arXiv:2502.16051}, year={2025}, url={https://arxiv.org/abs/2502.16051}}","bluf":"Presents a clinician-annotated dataset capturing real-world task ambiguity in mental healthcare, highlighting evaluation challenges beyond exam-style QA and demonstrating the need for richer evaluation protocols.","keywords":["dataset","ambiguity","evaluation","mental healthcare"],"top_claims":["Real-world tasks present ambiguity not captured by exam QA datasets","Clinician annotations enable more realistic evaluation of model helpfulness"],"url":"https://arxiv.org/abs/2502.16051"}
{"id":"grabb_2024_mentalhealth_risks","title":"Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation","authors":["Daniel Grabb","Max Lamparth","Neeraj Vasan"],"year":2024,"bibtex":"@article{grabb_lamparth_vasan_2024_mentalhealth, title={Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation}, author={Grabb, Daniel and Lamparth, Max and Vasan, Neeraj}, journal={arXiv preprint arXiv:2406.11852}, year={2024}, note={CoLM 2024}, url={https://arxiv.org/abs/2406.11852}}","bluf":"Surveys ethical and structural risks of deploying LLMs in mental-health contexts and proposes an implementation framework with guardrails to mitigate harm in sensitive, high-stakes deployments.","keywords":["ethics","mental healthcare","risks","deployment"],"top_claims":["LLMs pose special risks in automated mental health delivery that require structured governance","Proposes layered mitigations and evaluation structures for safe deployment"],"url":"https://arxiv.org/abs/2406.11852"}
