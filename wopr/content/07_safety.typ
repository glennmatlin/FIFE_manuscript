#import "../config.typ":num_papers
= Safety Considerations
// (Parv Yixiong)

The results of wargames often directly inform organizational policy, discussions, and institutional decision-making, including in sensitive policy and defensive contexts @uk_ministry_of_defense_influence_2023. Because the design and interpretation of wargames usually assume human players and adjudicators with meaningfully different behavior patterns than LMs, the incorporation of LMs in wargaming necessitates reevaluation of traditional wargaming methodology and interpretation @downes-martin_adjudication_2013. 

== Known LM Vulnerabilities

Prior work notes recurring issues when LMs are used for simulation, including bias, (lack of) diversity, and sycophancy @lamparth_human_2024 @anthis_llm_2025 @hammond_multi-agent_2025. These observations vary by model and motivate context-dependent guardrails rather than categorical claims about capability. We outline a non-exhaustive set of vulnerabilities as they apply to the most common applications of wargaming, but practitioners should carefully evaluate LM behavior in their own contexts.  
- *Escalation dynamics:* LMs have shown escalatory tendencies in diplomatic and military contexts @rivera_escalation_2024. Despite proposed mitigation techniques @elbaum_managing_2025, this is an important consideration for military and diplomatic wargaming use-cases.
- *Implicit bias:* Noise and spurious correlations in pre-training and post-training mean that LMs exhibit social biases despite prompting @taubenfeld_systematic_2024.  Furthermore, @mazeika_utility_2025 demonstrates that these biases can lead to implicit preferences for certain world states. Bias may lead to systematic blind spots in LMs agents, which is particularly concerning for adversarial modeling.    
- *Unfaithful reasoning:* @turpin_language_2023 @lanham_measuring_2023 show that LMs are not always 'faithful' in their reasoning, meaning their Chain-of-Thought (CoT) may not accurately describe how they arrived at their outputs; in the context of wargaming, this could manifest as misattribution of their decision factors.
- *Sycophancy:* Off-the-self LMs are trained to mimic assistants, but users prefer agreeable assistants, so a notable artifact of post-training is emergent sycophancy @sharma_towards_2025. In red-team or human-AI exercises, sycophancy may mask strategic vulnerabilities and incorrectly validate operator assumptions. 
- *Long context incoherence*: LMs struggle to maintain cohesion over long contexts @liu_lost_2023. Given there is also a gap between the claimed context and effective context lengths @modarressi_nolima_2025, and effective wargaming requires strategic continuity, long horizon simulations may require sound context compression strategies to remain effective.


 RIEDL: I wonder if there is another concern that has to do with sensitivity to system prompt. This is a form of biasing of outputs. I'll see if I can come up with an example or articulate this better. It's similar to how xAI biases Grok with their system prompt "be skeptical of trust mainstream media"))

//Parv: I think this is what intepretation of wargames was trying to get at... the prompting creates these weird recursive personas which are brittle

== Interpretation of wargames
// parv: Other ideas: Simulation Depth and Interpretive Challenges, "Contextual Brittleness", something something multi-level persona implications

((RIEDL: I'm unclear of what "interpretation of wargames" means or is getting at. I do think it is bringing out some more vulnerabilities. One seems to be stereotypes, which is maybe a variant on implicit biases, but applied to roles (the TV version of a soldier as opposed to what a soldier is really like). Another seems to be that post-training can run contrary to the goals of the wargame. The example I think that is being used here is training to be helpful, which is not always appropriate if the LM needs to be the red-team. Same with harmless and honest. Perhaps the thing to do is to lean into the HHH assumptions of post-training and talk about how implicit assumptions about HHH can mean wargames are not always good representations about how courses of action will unfold in the real world.))

// Parv: ok so this shows to me that we failed in framing this section correctly. i think this might be a title issue?  see above. need feedback and thoughts. So my idea to resolve this comment is to reframe this section through a better title. Thoughts??

Language models draw on in-context information and pretraining data to simulate a target distribution of text. These characters can be recursive; if, for instance, the model context instructs it to play a solider in a wargame, it will seek not to simulate the solider but instead a wargame's portrayal of a solider, drawing on text data from fictional portrayals and stylized military communications. Because instruction-tuned models are trained to adopt a helpful assistant persona (e.g., Claude, Qwen), it may in fact simulate an assistant portraying a wargamer who is in turn portraying a solider. Each level of simulation may produce unique and unexpected artifacts from that context's conventions. For this reason, interpreting LM outputs must acknowledge the context of these recursive personas, each of which is brittle to subtle prompting and contextual changes @lore_strategic_2024 @lutz_prompt_2025. Wargame practitioners using LMs should therefore ensure their results are replicable over diverse prompt formats, context structures, and wording changes. This LM-as-simulator framing implies that wargames using LM outputs for domain-specific tasks require human SME oversight @downes-martin_preference_2020 @freeman_ai_2024. Sophisticated-seeming responses may not reflect domain expertise, creating overconfidence in out-of-distribution scenarios where the model's training data provides poor guidance @taubenfeld_systematic_2024.
