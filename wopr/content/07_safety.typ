#import "../config.typ":num_papers
= Safety Considerations
// (Parv Yixiong)

The results of wargames often directly inform organizational policy, discussions, and institutional decision-making, including in sensitive policy and defensive contexts @uk_ministry_of_defense_influence_2023. Because the design and interpretation of wargames usually assume human players and adjudicators with meaningfully different behavior patterns than LMs, the incorporation of LMs in wargaming necessitates reevaluation of traditional wargaming methodology and interpretation @downes-martin_adjudication_2013. Prior work notes recurring issues when LMs are used for simulation, including bias, (lack of) diversity, and sycophancy @lamparth_human_2024 @anthis_llm_2025 @hammond_multi-agent_2025. These observations vary by model and motivate context-dependent guardrails rather than categorical claims about capability. We outline a non-exhaustive set of vulnerabilities as they apply to the most common applications of wargaming, but practitioners should carefully evaluate LM behavior in their own contexts.

- *Escalation dynamics:* LMs have shown escalatory tendencies in diplomatic and military contexts @rivera_escalation_2024. Despite proposed mitigation techniques @elbaum_managing_2025, this is an important consideration for military and diplomatic wargaming use-cases.
- *Implicit bias:* Noise and spurious correlations in pre-training and post-training mean that LMs exhibit social biases despite prompting @taubenfeld_systematic_2024.  Furthermore, @mazeika_utility_2025 demonstrates that these biases can lead to implicit preferences for certain world states. Bias may lead to systematic blind spots in LMs agents, which is particularly concerning for adversarial modeling.    
- *Unfaithful reasoning:* @turpin_language_2023 @lanham_measuring_2023 show that LMs are not always 'faithful' in their reasoning, meaning their Chain-of-Thought (CoT) may not accurately describe how they arrived at their outputs; in the context of wargaming, this could manifest as misattribution of their decision factors.
- *Sycophancy:* Off-the-self LMs are trained to mimic assistants, but users prefer agreeable assistants, so a notable artifact of post-training is emergent sycophancy @sharma_towards_2025. In red-team or human-AI exercises, sycophancy may mask strategic vulnerabilities and incorrectly validate operator assumptions. 
- *Long context incoherence*: LMs struggle to maintain cohesion over long contexts @liu_lost_2023. Given there is also a gap between the claimed context and effective context lengths @modarressi_nolima_2025, and effective wargaming requires strategic continuity, long horizon simulations may require sound context compression strategies to remain effective.
- *Recursive persona misgeneralization*: LMs draw on in-context information and pretraining data to simulate a target distribution of text. These characters can be recursive; if, for instance, the model context instructs it to play a solider in a wargame, it will seek not to simulate the solider but instead a wargame's portrayal of a solider, drawing on text data from fictional portrayals and stylized military communications. Because instruction-tuned models are trained to adopt a helpful assistant persona, it may in fact simulate an assistant portraying a wargamer who is in turn portraying a solider. Each level of simulation is brittle to subtle prompting and contextual changes, and may produce unique artifacts from that context's conventions @lore_strategic_2024 @lutz_prompt_2025. Sophisticated-seeming responses may not reflect domain expertise, creating overconfidence in out-of-distribution scenarios where the model's training data provides poor guidance @taubenfeld_systematic_2024.