= Appendix C: Detailed Safety Considerations <app:safety-details>

- *Escalation dynamics:* LMs have shown escalatory tendencies in diplomatic and military contexts @rivera_escalation_2024. Despite proposed mitigation techniques @elbaum_managing_2025, this is an important consideration for military and diplomatic wargaming use-cases.
- *Implicit bias:* Noise and spurious correlations in pre-training and post-training mean that LMs exhibit social biases despite prompting @taubenfeld_systematic_2024.  Furthermore, @mazeika_utility_2025 demonstrates that these biases can lead to implicit preferences for certain world states. Bias may lead to systematic blind spots in LMs agents, which is particularly concerning for adversarial modeling.    
- *Unfaithful reasoning:* @turpin_language_2023 @lanham_measuring_2023 show that LMs are not always 'faithful' in their reasoning, meaning their Chain-of-Thought (CoT) may not accurately describe how they arrived at their outputs; in the context of wargaming, this could manifest as misattribution of their decision factors.
- *Sycophancy:* Off-the-self LMs are trained to mimic assistants, but users prefer agreeable assistants, so a notable artifact of post-training is emergent sycophancy @sharma_towards_2025. In red-team or human-AI exercises, sycophancy may mask strategic vulnerabilities and incorrectly validate operator assumptions. 
- *Long context incoherence*: LMs struggle to maintain cohesion over long contexts @liu_lost_2023. Given there is also a gap between the claimed context and effective context lengths @modarressi_nolima_2025, and effective wargaming requires strategic continuity, long horizon simulations may require sound context compression strategies to remain effective.
- *Prompt sensitivity:* The behavior of LMs can be heavily influenced by the system prompt. This can be a way to bias the output of the model. For example, a model's response can change significantly based on whether it is prompted to be a "helpful assistant" or a "skeptical critic". This sensitivity to the prompt can be exploited to generate desired outcomes, which can be a vulnerability in a wargaming context.
- *Recursive persona misgeneralization*: LMs draw on in-context information and pretraining data to simulate a target distribution of text. These characters can be recursive; if, for instance, the model context instructs it to play a solider in a wargame, it will seek not to simulate the solider but instead a wargame's portrayal of a solider, drawing on text data from fictional portrayals and stylized military communications. Because instruction-tuned models are trained to adopt a helpful assistant persona, it may in fact simulate an assistant portraying a wargamer who is in turn portraying a solider. Each level of simulation is brittle to subtle prompting and contextual changes, and may produce unique artifacts from that context's conventions @lore_strategic_2024 @lutz_prompt_2025. Sophisticated-seeming responses may not reflect domain expertise, creating overconfidence in out-of-distribution scenarios where the model's training data provides poor guidance @taubenfeld_systematic_2024.