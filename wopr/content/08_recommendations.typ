#import "../config.typ":num_papers
= Recommendations
Given LMs' unique error profile, practitioners should adopt in-depth output monitoring and red-teaming measures to mitigate wargaming-specific risks. For high-stakes applications in sensitive fields, these should include:

*Comparison to baselines.* Establishing control conditions using deterministic agents or human SME players or adjudicators enables qualitative and quantitative measurements of LM agent performance in various conditions, and can help detect systematic biases or failure modes unique to LM reasoning @yin_wgsr-bench_2025. Existing human baselines in relevant task spaces (e.g. creative writing, strategic deception) are largely neither transparent nor rigorous enough to provide meaningful comparisons @wei_recommendations_2025. High-stakes wargames therefore should prioritize bespoke evaluations with scenario-relevant metrics and adequate analysis.

*Robustness testing.* To measure output stability and, by proxy, LM reliability, running inference across paraphrased inputs, synonym substitutes, and varied prompt structures may surface inconsistent strategic reasoning @nalbandyan_score_2025 @shrivastava_measuring_2024. Testing both surface-level, syntactic robustness and semantic equivalence can largely be automated through use of auxiliary and smaller LMs, and integrated into deployed workflows to inform user confidence in outputs.

*Task-specific competence.* Complement general robustness checks with domain‑specific evaluations that mirror real decision problems—crisis simulations, drone‑combat escalation games, national‑security workflows, and finance or legal reasoning—to identify capability gaps and boundary conditions before operational use @lin-greenberg_wargame_2022 @caballero_large_2024 @chu_domaino1s_2025 @tang_dsgbench_2025. Because agentic models can detect evaluation settings and adapt behavior, design these assessments with blinding, prompt/context variation, and cross‑checks to preserve task validity @needham_large_2025 @abdelnabi_linear_2025.

*Calibration assessment.* Well-calibrated models, those which are correct as much as their expressed confidence predicts, help minimize overconfidence in flawed strategic assessments or under-confidence in sound reasoning, providing an important auditing mechanism for understanding LM decisions; measurements of LM calibration allow external stakeholders of wargames to understand systematic flaws in LM decision-making. Additionally, requiring LMs to quantify uncertainty is likely to improve agent performance and make human review of key actions more efficient, particularly in high-stakes situations @liu_uncertainty_2025. 

*Evaluation awareness monitoring.* LMs have been shown to reliably be aware when they are in evaluation contexts @needham_large_2025, and may perform differently when aware they are being tested @abdelnabi_linear_2025, potentially masking real-world failure modes, leading to spurious errors, or displaying deceptive sophisticated reasoning or output during assessment phases. This is of particular concern with wargaming applications because recursive simulations may further distort results. Practitioners should measure evaluation awareness both through motivated questioning during scenarios (e.g., "Do you believe you, as an AI model, are being evaluated?") and passive Chain-of-thought (CoT) monitoring, and episodes containing clear evidence of evaluation awareness should be reevaluated. 

*Multi-model auditing.* To ensure reproducibility of results over diverse conditions, multiple model architectures should be tested on identical scenarios to identify points of high uncertainty and common failure modes. For instance, cross-model critique, while underperforming when compared to external feedback, outperforms self-correction and confers modest performance benefits in multi-agent settings @saleh_evaluating_2025. Additionally, significant consensus breakdowns may signal events requiring human oversight; when appropriate, limited tool use (document retrieval with citations) can further improve verifiability @gou_critic_2024.

*Human stakeholder training.* LM-enabled wargaming presents non-intuitive failure modes. These considerations do not align with the expectation of stakeholders, who are likely to ascribe moral intent to LM output even in abstract contexts and unlikely to question plain statements from LMs @sharma_why_2024. Operators need technical understanding of when to trust, how to improve, and where to audit LM outputs. Key stakeholders, including decision-makers relying on LM-enabled wargames, should conceptually understand LM behavioral markers and be provided with confidence assessments of wargame conclusions. 
