#import "../config.typ":num_papers
= Open Research Questions
Widespread language model deployment in open-ended wargaming scenarios necessitates incorporating sophisticated planning algorithms for long-term strategy in large state spaces, improved counterfactual reasoning for creative strategy generation, and robust world models for dynamic environments. These capabilities are essential for addressing the complex, partially-specified nature of open-ended wargaming scenarios where traditional computational approaches prove insufficient. Several research challenges require both theoretical and empirical investigation:

- *How can we reliably elicit and maintain coherent hierarchal personas without behavioral leakage between persona levels?* Persona misgeneralization may cause models to simulate stylized fictional portrayals rather than authentic domain expertise potentially masking deficiencies in real-world capability. One class of promising solutions involves intervening on model internals through activation steering, which leverages un-interpretable yet meaningful hidden states or personas @turner_steering_2024 @chalnev_improving_2024 @chen_persona_2025. However, such techniques remain highly brittle and unpredictable @tan_analyzing_2025 @hao_patterns_2025 and require further advancement.

- *Which interpretability methods can recover explanations for tactical decisions in multi-agent wargaming environments involving deception and opponent modeling?* CoT is often unfaithful and current interpretability methods do not generalize to hierarchical role structures @turpin_language_2023, making LM strategy opaque and difficult to recover strategic insights from. To avoid relying on CoT, researchers may train sparse autoencoders on game state representations to decompose strategic concepts into interpretable features @cunningham_sparse_2023, though this approach may inadequately capture the sequential nature of strategic reasoning. Activation patching offers an alternative method for causally validating which features drive deceptive behaviors, potentially providing experimental control over agent opponent modeling @ravindran_adversarial_2025.

- *What evaluation metrics can reliably compare agent strategic depth and long-term planning capability in open-ended wargames?* There exist no standard wargaming evaluation methods or protocols that measure agent ability over long tasks @reddie_next-generation_2018 @downes-martin_wargaming_2025 @reddie_wargames_2023, making it difficult to construct control conditions for Human-AI comparisons or quantitively assess performance. LM judges, while quickly improving in matching human ability, still exhibit unique cognitive biases over long task horizons @li_llms-as-judges_2024. Developing efficient methods to obtain subject matter expert performance metrics comparable to human benchmarks represents a valuable research direction @wei_recommendations_2025.

- *How can we mitigate brittle wargaming agent behavior in open-ended wargames when facing out-of-distribution scenarios?* Without sufficient robustness, agents may fail catastrophically when facing novel adersary attacks, creating false confidence, or trigger unrealistic behavior claims across other agents. While out-of-distribution detection methods have been explored in neural networks @liang_principled_2017, their generalizability to multi-agent settings and latent distributional shifts in transformer-based models remains unclear @smith_understanding_2024.