= Related Works
== Artificial Intelligence in Wargaming 
//@li_exploration_2025
//@davis_artificial_2025
Foundational defense wargaming literature establishes core definitions, taxonomies of formats
(seminar, matrix, kriegsspiel, computer‑assisted), and adjudication philosophies (rigid rules vs.
free adjudication) @us_army_war_college_strategic_2015 @deutchland_bundeswehr_wargaming_2017 @uk_ministry_of_defense_wargaming_2017. These works emphasize
intent, uncertainty management, and the central role of facilitation and expert judgment in
producing insight rather than scorekeeping. Practitioner handbooks and RAND‑style studies
standardize scenario design, injects, decision capture, and after‑action analysis. Together, this
tradition motivates our focus on language‑based, argument‑centric play and clarifies where
automation must respect human roles.

Early AI for wargaming largely targeted quantitative or tightly scoped simulations: search and
optimization for move planning, stochastic combat models, and agent‑based systems with
explicit state. Such approaches perform well in rigid games but offer limited support for
open‑ended narrative argumentation and multi‑party persuasion typical of seminar and matrix formats.

== Literature Reviews on LMs in Games
// CITE: LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models
// A Survey on Large Language Model-Based Game Agents
// Large Language Models and Games: A Survey and Roadmap
// LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models
// Playing games with Large language models: Randomness and strategy
// SPIN-Bench: How well do LLMs plan strategically and reason socially?
// A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges
// A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios
// Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis

Surveys of LMs in games and simulation describe agents that negotiate, plan, or role‑play in
constrained environments @ma_computational_2024 @zhang_llm_2024 work on Diplomacy combines
language models with planning and tool use @meta_fundamental_ai_research_diplomacy_team_fair_human-level_2022 @lamparth_human_2024 and multi‑agent LM frameworks
explore coordination, debate, and self‑critique @zhang_llm_2024 @yao_spin-bench_2025. Parallel reviews
in defense communities catalog AI for training and analysis but emphasize quantitative or kinetic
models.
Across these threads, reporting is heterogeneous and evaluation often short‑horizon or
subjective; prompts and facilitation procedures are rarely standardized; and open‑ended,
qualitative wargames remain under‑served. Our discussion consolidates approaches for
language‑based formats and highlights practical choices that affect realism and utility.
