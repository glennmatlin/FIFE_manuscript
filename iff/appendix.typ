#import "@preview/pintorita:0.1.4": render

= Appendix: IFF System Architecture

The IFF evaluation framework is built on a modular, extensible architecture designed to ensure maintainability and scalability. The design philosophy emphasizes a clear separation of concerns, allowing for independent development and testing of its core components.

== A.1 Layered Architecture

The system is organized into four distinct logical layers:

1.  **Application Layer:** This is the top-most layer that provides the user interface for interacting with the framework. It includes command-line scripts such as `evaluation_bin.py` for running evaluations and `generate_responses.py` for generating model outputs.
2.  **Business Logic Layer:** This layer contains the core functionality of the framework. It includes the `evaluation_lib.py` module, which orchestrates the evaluation process, and the instruction modules (`finance_instructions`, `instructions_registry`) that define the logic for each verifiable constraint.
3.  **Utility Layer:** This layer provides common, reusable functions that support the business logic. The `instructions_util.py` module resides here, offering text processing and validation functions (e.g., word counting, sentence splitting, table detection) that are used by multiple instruction checkers.
4.  **Data Layer:** This layer consists of the data artifacts used and generated by the framework, including the input `JSONL` files containing the prompts and the output `JSONL` files containing model responses and evaluation results.

== A.2 Core Components & Data Flow

The framework's operation is driven by the interaction between its key modules, as illustrated in the component dependency diagram.

#figure(
  render(
    """
    graph TD
        A[evaluation_bin] --> B[evaluation_lib]
        B --> C[instructions_registry]
        C --> D[finance_instructions]
        D --> F[instructions_util]
        G[build_input_jsonl] --> C
        H[generate_responses] --> I[External API]
    """,
    kind: "graphviz",
  ),
  caption: [Component Dependency Diagram]
)

-   **`build_input_jsonl`**: This script generates the benchmark's test cases. It accesses the `instructions_registry` to combine various instruction types and parameters into complex financial prompts, which are then saved to a `JSONL` file.
-   **`generate_responses`**: This module takes the generated prompts and uses its integrated, multi-provider LLM Gateway to query external models (e.g., GPT-4, Claude 3). It handles API communication, error handling, and caching, saving the model-generated text into a responses `JSONL` file.
-   **`evaluation_bin`**: This is the main evaluation runner. It uses `evaluation_lib` to load the prompts and their corresponding responses.
-   **`evaluation_lib`**: The core evaluation engine. For each prompt, it retrieves the required instruction-checking logic from the `instructions_registry`. It then executes the "Strict" or "Loose" evaluation algorithm against the model's response.
-   **`instructions_registry`**: A central mapping that holds all available instruction-checking classes. This registry pattern allows for easy discovery and instantiation of the correct validation logic based on an instruction's unique ID.
-   **`finance_instructions`**: This module contains the implementation for each specific financial instruction (e.g., `FinBoldIntroItalicRisk`, `FinCreditSpreadCarryTable`). Each checker inherits from a base class and implements the specific logic required to validate its constraint.
-   **`instructions_util`**: Provides foundational text-processing functions (e.g., `count_words`, `has_table`) used by the individual instruction checkers in `finance_instructions`.

== A.3 Design Patterns

The architecture leverages several key software design patterns to enhance its flexibility and maintainability:

-   **Registry Pattern:** The `instructions_registry` acts as a central point of contact for all instruction checkers, decoupling the evaluation engine from the concrete implementation of the checkers.
-   **Strategy Pattern:** The use of "Strict" and "Loose" evaluation modes is a classic example of the Strategy pattern. The main evaluation library can switch between these different evaluation algorithms at runtime without changing its core logic.
-   **Factory Pattern:** The framework uses a factory approach to dynamically create instances of instruction-checker classes from the registry based on the IDs specified in a given prompt.