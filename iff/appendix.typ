#import "@preview/fletcher:0.5.8": diagram, node, edge

= Appendix: IFF System Architecture

The IFF evaluation framework is built on a modular, extensible architecture designed to ensure maintainability and scalability. The design philosophy emphasizes a clear separation of concerns, allowing for independent development and testing of its core components.

== A.1 Layered Architecture

The system is organized into four distinct logical layers:

1.  *Application Layer:* This is the top-most layer that provides the user interface for interacting with the framework. It includes command-line scripts such as `evaluation_bin.py` for running evaluations and `generate_responses.py` for generating model outputs.
2.  *Business Logic Layer:* This layer contains the core functionality of the framework. It includes the `evaluation_lib.py` module, which orchestrates the evaluation process, and the instruction modules (`finance_instructions`, `instructions_registry`) that define the logic for each verifiable constraint.
3.  *Utility Layer:* This layer provides common, reusable functions that support the business logic. The `instructions_util.py` module resides here, offering text processing and validation functions (e.g., word counting, sentence splitting, table detection) that are used by multiple instruction checkers.
4.  *Data Layer:* This layer consists of the data artifacts used and generated by the framework, including the input `JSONL` files containing the prompts and the output `JSONL` files containing model responses and evaluation results.

== A.2 Core Components & Data Flow

The framework's operation is driven by the interaction between its key modules, as illustrated in the component dependency diagram.

#figure(
  diagram(
    cell-size: (40mm, 20mm),
    edge-stroke: 0.8pt,
    edge-corner-radius: 4pt,
    mark-scale: 90%,

    // Nodes
    component((0, 0), [build_input_jsonl]),
    component((0, 1), [evaluation_bin]),
    component((0, 2), [generate_responses]),

    component((1, 1), [evaluation_lib]),
    component((1, 2), [External API]),

    component((2, 1), [instructions_registry]),
    component((3, 1), [finance_instructions]),
    component((4, 1), [instructions_util]),

    // Edges
    edge((0, 1), (1, 1), "-|>"), 
    edge((1, 1), (2, 1), "-|>"), 
    edge((0, 0), (2, 1), "-|>"), 
    edge((0, 2), (1, 2), "-|>"), 
    edge((2, 1), (3, 1), "-|>"), 
    edge((3, 1), (4, 1), "-|>"),
  ),
  caption: [Component Dependency Diagram]
)

-   *`build_input_jsonl`*: This script generates the benchmark's test cases. It accesses the `instructions_registry` to combine various instruction types and parameters into complex financial prompts, which are then saved to a `JSONL` file.
-   *`generate_responses`*: This module takes the generated prompts and uses its integrated, multi-provider LM Gateway to query external models (e.g., GPT-4, Claude 3). It handles API communication, error handling, and caching, saving the model-generated text into a responses `JSONL` file.
-   *`evaluation_bin`*: This is the main evaluation runner. It uses `evaluation_lib` to load the prompts and their corresponding responses.
-   *`evaluation_lib`*: The core evaluation engine. For each prompt, it retrieves the required instruction-checking logic from the `instructions_registry`. It then executes the "Strict" or "Loose" evaluation algorithm against the model's response.
-   *`instructions_registry`*: A central mapping that holds all available instruction-checking classes. This registry pattern allows for easy discovery and instantiation of the correct validation logic based on an instruction's unique ID.
-   *`finance_instructions`*: This module contains the implementation for each specific financial instruction (e.g., `FinBoldIntroItalicRisk`, `FinCreditSpreadCarryTable`). Each checker inherits from a base class and implements the specific logic required to validate its constraint.
-   *`instructions_util`*: Provides foundational text-processing functions (e.g., `count_words`, `has_table`) used by the individual instruction checkers in `finance_instructions`.

== A.3 Design Patterns

The architecture leverages several key software design patterns to enhance its flexibility and maintainability:

-   *Registry Pattern:* The `instructions_registry` acts as a central point of contact for all instruction checkers, decoupling the evaluation engine from the concrete implementation of the checkers.
-   *Strategy Pattern:* The use of "Strict" and "Loose" evaluation modes is a classic example of the Strategy pattern. The main evaluation library can switch between these different evaluation algorithms at runtime without changing its core logic.
-   *Factory Pattern:* The framework uses a factory approach to dynamically create instances of instruction-checker classes from the registry based on the IDs specified in a given prompt.
