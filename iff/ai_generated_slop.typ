
// - *Numerical Reasoning.* We find this to be the most challenging category for all models. We observe the lowest success rates for tasks requiring multi-step calculations from provided data. Even GPT-4 only achieves about 60-65% accuracy here. We find that models frequently make arithmetic mistakes, mis-place decimals, or produce plausible-sounding but incorrect numbersâ€”a dangerous tendency in finance.
// - *Extraction and Formatting.* We find that models are generally proficient at extracting information and adhering to structured output formats like JSON or bulleted lists. GPT-4 and Claude exceed 85% accuracy. We observe that the primary failure mode for other models is partial compliance, such as mixing explanatory text with the requested JSON object.
// - *Analytical Reasoning.* We find that GPT-4 and Claude 2 excel at prompts requiring a chain of reasoning based on domain knowledge. They produce well-structured and coherent analyses over 80% of the time. We observe that open-source models often provide relevant terminology but sometimes fail to structure the answer clearly or directly address all parts of the query.
// - *Compliance and Constraints.* We observed a sharp difference in alignment when testing precise constraints (e.g., "List _exactly three_ impacts"). GPT-4 and Claude almost always respected these constraints, whereas other models frequently violated them by providing more or fewer items than requested. This demonstrates the effectiveness of extensive RLHF in teaching models to obey fine-grained instructions.

// == Error Analysis
// We identify several common error types:
// - *Calculation Mistakes:* The most frequent error type, especially for models without integrated tool use.
// - *Ignoring Part of an Instruction:* Models often addressed only the primary part of a multi-part prompt, neglecting secondary constraints or requests for explanation.
// - *Hallucinated Content:* In a few instances, models introduced facts or figures not present in the provided context, a critical failure mode for data-driven financial tasks.
// - *Surface-Level Deception:* In some cases, models produced answers that were well-formatted and fluent but factually incorrect or non-compliant. This highlights the risk of 'surface-level deception', where an LM-as-a-judge might be swayed by style over substance @ye_flask_2023, reinforcing the need for multi-faceted evaluation.
// - *Formatting Issues.* We find that a notable portion of errors result from failing to adhere to output formats, such as providing a conversational answer when a structured one was requested.

// == The Instruction Hierarchy in Finance

// We also observe a tension in what we term the "instruction hierarchy." When faced with a complex analytical task (e.g., summarizing market risks) and a strict, verifiable constraint (e.g., "the summary must be exactly 75 words"), models must make a trade-off. We find that some models, particularly those highly tuned for conversational quality, often prioritize producing a fluent, high-quality analysis while failing the length constraint. Conversely, we find that other models, when trained specifically on constraint following, sometimes sacrifice the quality or coherence of the analysis to meet the verifiable requirement precisely. We believe this highlights a key challenge for the development of financial assistants: building models that can gracefully balance the primary analytical task with strict adherence to secondary constraints, without sacrificing one for the other.


// == Framework (Glenn can we rid of this part or any other subseciton in this section, this is going too long)
// We support our evaluation protocol with a robust and extensible framework that we designed for comprehensive research. To facilitate the evaluation of a wide array of language models, we use the `litellm` library to act as our gateway for LM providers, which lets us compare models from various sources. For every API call, we automatically record all metrics, including token usage (prompt, completion, and total), API latency in milliseconds, and the financial cost of the call. We save this metadata alongside the evaluation results for post-generation analysis. The framework is centered around the central instruction registry which is where definitions of instruction types and verification methods are stored. Researchers can add new and even more complex instructions by implementing a simple checker class. 



// We observe a wide performance gap between top-tier proprietary models and their open-source counterparts. We find that ??????? is the clear leader, achieving an overall accuracy of approximately ???. Both ????? and ????? follow, with scores in the ????? range. ????lags behind at roughly ????, which confirms that the latest generation of models has made significant strides in handling complex instructions. Among open-source models, ???? is the top performer in instruction compliance, reaching ???? strict accuracy. When we analyze across each instruction type, we get a more granular view of model behavior. Here, ??? again demonstrates superior performance. We find that it achieves ??? prompt-level accuracy in strict mode and ??? in loose mode. This indicates that it follows instructions almost perfectly, with only minor, infrequent lapses. ??? also performs strongly, with around ??? strict prompt accuracy. The gap is more pronounced for other models. ??? achieves a respectable ??? strict prompt-level compliance, but this is still ??? points behind ???. We find that models often fail to adhere to the precise formatting or structural requirements of an instruction. The larger gap between strict and loose scores for open-source models indicates that many of their failures are "almost correct" outputs that miss a specific detail.
